encoder.model.conv_stem.weight: torch.Size([32, 15, 3, 3])
encoder.model.bn1.weight: torch.Size([32])
encoder.model.bn1.bias: torch.Size([32])
encoder.model.bn1.running_mean: torch.Size([32])
encoder.model.bn1.running_var: torch.Size([32])
encoder.model.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.0.0.conv.weight: torch.Size([32, 32, 3, 3])
encoder.model.blocks.0.0.bn1.weight: torch.Size([32])
encoder.model.blocks.0.0.bn1.bias: torch.Size([32])
encoder.model.blocks.0.0.bn1.running_mean: torch.Size([32])
encoder.model.blocks.0.0.bn1.running_var: torch.Size([32])
encoder.model.blocks.0.0.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.0.1.conv.weight: torch.Size([32, 32, 3, 3])
encoder.model.blocks.0.1.bn1.weight: torch.Size([32])
encoder.model.blocks.0.1.bn1.bias: torch.Size([32])
encoder.model.blocks.0.1.bn1.running_mean: torch.Size([32])
encoder.model.blocks.0.1.bn1.running_var: torch.Size([32])
encoder.model.blocks.0.1.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.0.2.conv.weight: torch.Size([32, 32, 3, 3])
encoder.model.blocks.0.2.bn1.weight: torch.Size([32])
encoder.model.blocks.0.2.bn1.bias: torch.Size([32])
encoder.model.blocks.0.2.bn1.running_mean: torch.Size([32])
encoder.model.blocks.0.2.bn1.running_var: torch.Size([32])
encoder.model.blocks.0.2.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.0.3.conv.weight: torch.Size([32, 32, 3, 3])
encoder.model.blocks.0.3.bn1.weight: torch.Size([32])
encoder.model.blocks.0.3.bn1.bias: torch.Size([32])
encoder.model.blocks.0.3.bn1.running_mean: torch.Size([32])
encoder.model.blocks.0.3.bn1.running_var: torch.Size([32])
encoder.model.blocks.0.3.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.1.0.conv_exp.weight: torch.Size([128, 32, 3, 3])
encoder.model.blocks.1.0.bn1.weight: torch.Size([128])
encoder.model.blocks.1.0.bn1.bias: torch.Size([128])
encoder.model.blocks.1.0.bn1.running_mean: torch.Size([128])
encoder.model.blocks.1.0.bn1.running_var: torch.Size([128])
encoder.model.blocks.1.0.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.1.0.conv_pwl.weight: torch.Size([64, 128, 1, 1])
encoder.model.blocks.1.0.bn2.weight: torch.Size([64])
encoder.model.blocks.1.0.bn2.bias: torch.Size([64])
encoder.model.blocks.1.0.bn2.running_mean: torch.Size([64])
encoder.model.blocks.1.0.bn2.running_var: torch.Size([64])
encoder.model.blocks.1.0.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.1.1.conv_exp.weight: torch.Size([256, 64, 3, 3])
encoder.model.blocks.1.1.bn1.weight: torch.Size([256])
encoder.model.blocks.1.1.bn1.bias: torch.Size([256])
encoder.model.blocks.1.1.bn1.running_mean: torch.Size([256])
encoder.model.blocks.1.1.bn1.running_var: torch.Size([256])
encoder.model.blocks.1.1.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.1.1.conv_pwl.weight: torch.Size([64, 256, 1, 1])
encoder.model.blocks.1.1.bn2.weight: torch.Size([64])
encoder.model.blocks.1.1.bn2.bias: torch.Size([64])
encoder.model.blocks.1.1.bn2.running_mean: torch.Size([64])
encoder.model.blocks.1.1.bn2.running_var: torch.Size([64])
encoder.model.blocks.1.1.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.1.2.conv_exp.weight: torch.Size([256, 64, 3, 3])
encoder.model.blocks.1.2.bn1.weight: torch.Size([256])
encoder.model.blocks.1.2.bn1.bias: torch.Size([256])
encoder.model.blocks.1.2.bn1.running_mean: torch.Size([256])
encoder.model.blocks.1.2.bn1.running_var: torch.Size([256])
encoder.model.blocks.1.2.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.1.2.conv_pwl.weight: torch.Size([64, 256, 1, 1])
encoder.model.blocks.1.2.bn2.weight: torch.Size([64])
encoder.model.blocks.1.2.bn2.bias: torch.Size([64])
encoder.model.blocks.1.2.bn2.running_mean: torch.Size([64])
encoder.model.blocks.1.2.bn2.running_var: torch.Size([64])
encoder.model.blocks.1.2.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.1.3.conv_exp.weight: torch.Size([256, 64, 3, 3])
encoder.model.blocks.1.3.bn1.weight: torch.Size([256])
encoder.model.blocks.1.3.bn1.bias: torch.Size([256])
encoder.model.blocks.1.3.bn1.running_mean: torch.Size([256])
encoder.model.blocks.1.3.bn1.running_var: torch.Size([256])
encoder.model.blocks.1.3.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.1.3.conv_pwl.weight: torch.Size([64, 256, 1, 1])
encoder.model.blocks.1.3.bn2.weight: torch.Size([64])
encoder.model.blocks.1.3.bn2.bias: torch.Size([64])
encoder.model.blocks.1.3.bn2.running_mean: torch.Size([64])
encoder.model.blocks.1.3.bn2.running_var: torch.Size([64])
encoder.model.blocks.1.3.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.1.4.conv_exp.weight: torch.Size([256, 64, 3, 3])
encoder.model.blocks.1.4.bn1.weight: torch.Size([256])
encoder.model.blocks.1.4.bn1.bias: torch.Size([256])
encoder.model.blocks.1.4.bn1.running_mean: torch.Size([256])
encoder.model.blocks.1.4.bn1.running_var: torch.Size([256])
encoder.model.blocks.1.4.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.1.4.conv_pwl.weight: torch.Size([64, 256, 1, 1])
encoder.model.blocks.1.4.bn2.weight: torch.Size([64])
encoder.model.blocks.1.4.bn2.bias: torch.Size([64])
encoder.model.blocks.1.4.bn2.running_mean: torch.Size([64])
encoder.model.blocks.1.4.bn2.running_var: torch.Size([64])
encoder.model.blocks.1.4.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.1.5.conv_exp.weight: torch.Size([256, 64, 3, 3])
encoder.model.blocks.1.5.bn1.weight: torch.Size([256])
encoder.model.blocks.1.5.bn1.bias: torch.Size([256])
encoder.model.blocks.1.5.bn1.running_mean: torch.Size([256])
encoder.model.blocks.1.5.bn1.running_var: torch.Size([256])
encoder.model.blocks.1.5.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.1.5.conv_pwl.weight: torch.Size([64, 256, 1, 1])
encoder.model.blocks.1.5.bn2.weight: torch.Size([64])
encoder.model.blocks.1.5.bn2.bias: torch.Size([64])
encoder.model.blocks.1.5.bn2.running_mean: torch.Size([64])
encoder.model.blocks.1.5.bn2.running_var: torch.Size([64])
encoder.model.blocks.1.5.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.1.6.conv_exp.weight: torch.Size([256, 64, 3, 3])
encoder.model.blocks.1.6.bn1.weight: torch.Size([256])
encoder.model.blocks.1.6.bn1.bias: torch.Size([256])
encoder.model.blocks.1.6.bn1.running_mean: torch.Size([256])
encoder.model.blocks.1.6.bn1.running_var: torch.Size([256])
encoder.model.blocks.1.6.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.1.6.conv_pwl.weight: torch.Size([64, 256, 1, 1])
encoder.model.blocks.1.6.bn2.weight: torch.Size([64])
encoder.model.blocks.1.6.bn2.bias: torch.Size([64])
encoder.model.blocks.1.6.bn2.running_mean: torch.Size([64])
encoder.model.blocks.1.6.bn2.running_var: torch.Size([64])
encoder.model.blocks.1.6.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.2.0.conv_exp.weight: torch.Size([256, 64, 3, 3])
encoder.model.blocks.2.0.bn1.weight: torch.Size([256])
encoder.model.blocks.2.0.bn1.bias: torch.Size([256])
encoder.model.blocks.2.0.bn1.running_mean: torch.Size([256])
encoder.model.blocks.2.0.bn1.running_var: torch.Size([256])
encoder.model.blocks.2.0.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.2.0.conv_pwl.weight: torch.Size([96, 256, 1, 1])
encoder.model.blocks.2.0.bn2.weight: torch.Size([96])
encoder.model.blocks.2.0.bn2.bias: torch.Size([96])
encoder.model.blocks.2.0.bn2.running_mean: torch.Size([96])
encoder.model.blocks.2.0.bn2.running_var: torch.Size([96])
encoder.model.blocks.2.0.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.2.1.conv_exp.weight: torch.Size([384, 96, 3, 3])
encoder.model.blocks.2.1.bn1.weight: torch.Size([384])
encoder.model.blocks.2.1.bn1.bias: torch.Size([384])
encoder.model.blocks.2.1.bn1.running_mean: torch.Size([384])
encoder.model.blocks.2.1.bn1.running_var: torch.Size([384])
encoder.model.blocks.2.1.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.2.1.conv_pwl.weight: torch.Size([96, 384, 1, 1])
encoder.model.blocks.2.1.bn2.weight: torch.Size([96])
encoder.model.blocks.2.1.bn2.bias: torch.Size([96])
encoder.model.blocks.2.1.bn2.running_mean: torch.Size([96])
encoder.model.blocks.2.1.bn2.running_var: torch.Size([96])
encoder.model.blocks.2.1.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.2.2.conv_exp.weight: torch.Size([384, 96, 3, 3])
encoder.model.blocks.2.2.bn1.weight: torch.Size([384])
encoder.model.blocks.2.2.bn1.bias: torch.Size([384])
encoder.model.blocks.2.2.bn1.running_mean: torch.Size([384])
encoder.model.blocks.2.2.bn1.running_var: torch.Size([384])
encoder.model.blocks.2.2.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.2.2.conv_pwl.weight: torch.Size([96, 384, 1, 1])
encoder.model.blocks.2.2.bn2.weight: torch.Size([96])
encoder.model.blocks.2.2.bn2.bias: torch.Size([96])
encoder.model.blocks.2.2.bn2.running_mean: torch.Size([96])
encoder.model.blocks.2.2.bn2.running_var: torch.Size([96])
encoder.model.blocks.2.2.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.2.3.conv_exp.weight: torch.Size([384, 96, 3, 3])
encoder.model.blocks.2.3.bn1.weight: torch.Size([384])
encoder.model.blocks.2.3.bn1.bias: torch.Size([384])
encoder.model.blocks.2.3.bn1.running_mean: torch.Size([384])
encoder.model.blocks.2.3.bn1.running_var: torch.Size([384])
encoder.model.blocks.2.3.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.2.3.conv_pwl.weight: torch.Size([96, 384, 1, 1])
encoder.model.blocks.2.3.bn2.weight: torch.Size([96])
encoder.model.blocks.2.3.bn2.bias: torch.Size([96])
encoder.model.blocks.2.3.bn2.running_mean: torch.Size([96])
encoder.model.blocks.2.3.bn2.running_var: torch.Size([96])
encoder.model.blocks.2.3.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.2.4.conv_exp.weight: torch.Size([384, 96, 3, 3])
encoder.model.blocks.2.4.bn1.weight: torch.Size([384])
encoder.model.blocks.2.4.bn1.bias: torch.Size([384])
encoder.model.blocks.2.4.bn1.running_mean: torch.Size([384])
encoder.model.blocks.2.4.bn1.running_var: torch.Size([384])
encoder.model.blocks.2.4.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.2.4.conv_pwl.weight: torch.Size([96, 384, 1, 1])
encoder.model.blocks.2.4.bn2.weight: torch.Size([96])
encoder.model.blocks.2.4.bn2.bias: torch.Size([96])
encoder.model.blocks.2.4.bn2.running_mean: torch.Size([96])
encoder.model.blocks.2.4.bn2.running_var: torch.Size([96])
encoder.model.blocks.2.4.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.2.5.conv_exp.weight: torch.Size([384, 96, 3, 3])
encoder.model.blocks.2.5.bn1.weight: torch.Size([384])
encoder.model.blocks.2.5.bn1.bias: torch.Size([384])
encoder.model.blocks.2.5.bn1.running_mean: torch.Size([384])
encoder.model.blocks.2.5.bn1.running_var: torch.Size([384])
encoder.model.blocks.2.5.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.2.5.conv_pwl.weight: torch.Size([96, 384, 1, 1])
encoder.model.blocks.2.5.bn2.weight: torch.Size([96])
encoder.model.blocks.2.5.bn2.bias: torch.Size([96])
encoder.model.blocks.2.5.bn2.running_mean: torch.Size([96])
encoder.model.blocks.2.5.bn2.running_var: torch.Size([96])
encoder.model.blocks.2.5.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.2.6.conv_exp.weight: torch.Size([384, 96, 3, 3])
encoder.model.blocks.2.6.bn1.weight: torch.Size([384])
encoder.model.blocks.2.6.bn1.bias: torch.Size([384])
encoder.model.blocks.2.6.bn1.running_mean: torch.Size([384])
encoder.model.blocks.2.6.bn1.running_var: torch.Size([384])
encoder.model.blocks.2.6.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.2.6.conv_pwl.weight: torch.Size([96, 384, 1, 1])
encoder.model.blocks.2.6.bn2.weight: torch.Size([96])
encoder.model.blocks.2.6.bn2.bias: torch.Size([96])
encoder.model.blocks.2.6.bn2.running_mean: torch.Size([96])
encoder.model.blocks.2.6.bn2.running_var: torch.Size([96])
encoder.model.blocks.2.6.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.3.0.conv_pw.weight: torch.Size([384, 96, 1, 1])
encoder.model.blocks.3.0.bn1.weight: torch.Size([384])
encoder.model.blocks.3.0.bn1.bias: torch.Size([384])
encoder.model.blocks.3.0.bn1.running_mean: torch.Size([384])
encoder.model.blocks.3.0.bn1.running_var: torch.Size([384])
encoder.model.blocks.3.0.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.3.0.conv_dw.weight: torch.Size([384, 1, 3, 3])
encoder.model.blocks.3.0.bn2.weight: torch.Size([384])
encoder.model.blocks.3.0.bn2.bias: torch.Size([384])
encoder.model.blocks.3.0.bn2.running_mean: torch.Size([384])
encoder.model.blocks.3.0.bn2.running_var: torch.Size([384])
encoder.model.blocks.3.0.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.3.0.se.conv_reduce.weight: torch.Size([24, 384, 1, 1])
encoder.model.blocks.3.0.se.conv_reduce.bias: torch.Size([24])
encoder.model.blocks.3.0.se.conv_expand.weight: torch.Size([384, 24, 1, 1])
encoder.model.blocks.3.0.se.conv_expand.bias: torch.Size([384])
encoder.model.blocks.3.0.conv_pwl.weight: torch.Size([192, 384, 1, 1])
encoder.model.blocks.3.0.bn3.weight: torch.Size([192])
encoder.model.blocks.3.0.bn3.bias: torch.Size([192])
encoder.model.blocks.3.0.bn3.running_mean: torch.Size([192])
encoder.model.blocks.3.0.bn3.running_var: torch.Size([192])
encoder.model.blocks.3.0.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.3.1.conv_pw.weight: torch.Size([768, 192, 1, 1])
encoder.model.blocks.3.1.bn1.weight: torch.Size([768])
encoder.model.blocks.3.1.bn1.bias: torch.Size([768])
encoder.model.blocks.3.1.bn1.running_mean: torch.Size([768])
encoder.model.blocks.3.1.bn1.running_var: torch.Size([768])
encoder.model.blocks.3.1.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.3.1.conv_dw.weight: torch.Size([768, 1, 3, 3])
encoder.model.blocks.3.1.bn2.weight: torch.Size([768])
encoder.model.blocks.3.1.bn2.bias: torch.Size([768])
encoder.model.blocks.3.1.bn2.running_mean: torch.Size([768])
encoder.model.blocks.3.1.bn2.running_var: torch.Size([768])
encoder.model.blocks.3.1.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.3.1.se.conv_reduce.weight: torch.Size([48, 768, 1, 1])
encoder.model.blocks.3.1.se.conv_reduce.bias: torch.Size([48])
encoder.model.blocks.3.1.se.conv_expand.weight: torch.Size([768, 48, 1, 1])
encoder.model.blocks.3.1.se.conv_expand.bias: torch.Size([768])
encoder.model.blocks.3.1.conv_pwl.weight: torch.Size([192, 768, 1, 1])
encoder.model.blocks.3.1.bn3.weight: torch.Size([192])
encoder.model.blocks.3.1.bn3.bias: torch.Size([192])
encoder.model.blocks.3.1.bn3.running_mean: torch.Size([192])
encoder.model.blocks.3.1.bn3.running_var: torch.Size([192])
encoder.model.blocks.3.1.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.3.2.conv_pw.weight: torch.Size([768, 192, 1, 1])
encoder.model.blocks.3.2.bn1.weight: torch.Size([768])
encoder.model.blocks.3.2.bn1.bias: torch.Size([768])
encoder.model.blocks.3.2.bn1.running_mean: torch.Size([768])
encoder.model.blocks.3.2.bn1.running_var: torch.Size([768])
encoder.model.blocks.3.2.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.3.2.conv_dw.weight: torch.Size([768, 1, 3, 3])
encoder.model.blocks.3.2.bn2.weight: torch.Size([768])
encoder.model.blocks.3.2.bn2.bias: torch.Size([768])
encoder.model.blocks.3.2.bn2.running_mean: torch.Size([768])
encoder.model.blocks.3.2.bn2.running_var: torch.Size([768])
encoder.model.blocks.3.2.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.3.2.se.conv_reduce.weight: torch.Size([48, 768, 1, 1])
encoder.model.blocks.3.2.se.conv_reduce.bias: torch.Size([48])
encoder.model.blocks.3.2.se.conv_expand.weight: torch.Size([768, 48, 1, 1])
encoder.model.blocks.3.2.se.conv_expand.bias: torch.Size([768])
encoder.model.blocks.3.2.conv_pwl.weight: torch.Size([192, 768, 1, 1])
encoder.model.blocks.3.2.bn3.weight: torch.Size([192])
encoder.model.blocks.3.2.bn3.bias: torch.Size([192])
encoder.model.blocks.3.2.bn3.running_mean: torch.Size([192])
encoder.model.blocks.3.2.bn3.running_var: torch.Size([192])
encoder.model.blocks.3.2.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.3.3.conv_pw.weight: torch.Size([768, 192, 1, 1])
encoder.model.blocks.3.3.bn1.weight: torch.Size([768])
encoder.model.blocks.3.3.bn1.bias: torch.Size([768])
encoder.model.blocks.3.3.bn1.running_mean: torch.Size([768])
encoder.model.blocks.3.3.bn1.running_var: torch.Size([768])
encoder.model.blocks.3.3.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.3.3.conv_dw.weight: torch.Size([768, 1, 3, 3])
encoder.model.blocks.3.3.bn2.weight: torch.Size([768])
encoder.model.blocks.3.3.bn2.bias: torch.Size([768])
encoder.model.blocks.3.3.bn2.running_mean: torch.Size([768])
encoder.model.blocks.3.3.bn2.running_var: torch.Size([768])
encoder.model.blocks.3.3.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.3.3.se.conv_reduce.weight: torch.Size([48, 768, 1, 1])
encoder.model.blocks.3.3.se.conv_reduce.bias: torch.Size([48])
encoder.model.blocks.3.3.se.conv_expand.weight: torch.Size([768, 48, 1, 1])
encoder.model.blocks.3.3.se.conv_expand.bias: torch.Size([768])
encoder.model.blocks.3.3.conv_pwl.weight: torch.Size([192, 768, 1, 1])
encoder.model.blocks.3.3.bn3.weight: torch.Size([192])
encoder.model.blocks.3.3.bn3.bias: torch.Size([192])
encoder.model.blocks.3.3.bn3.running_mean: torch.Size([192])
encoder.model.blocks.3.3.bn3.running_var: torch.Size([192])
encoder.model.blocks.3.3.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.3.4.conv_pw.weight: torch.Size([768, 192, 1, 1])
encoder.model.blocks.3.4.bn1.weight: torch.Size([768])
encoder.model.blocks.3.4.bn1.bias: torch.Size([768])
encoder.model.blocks.3.4.bn1.running_mean: torch.Size([768])
encoder.model.blocks.3.4.bn1.running_var: torch.Size([768])
encoder.model.blocks.3.4.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.3.4.conv_dw.weight: torch.Size([768, 1, 3, 3])
encoder.model.blocks.3.4.bn2.weight: torch.Size([768])
encoder.model.blocks.3.4.bn2.bias: torch.Size([768])
encoder.model.blocks.3.4.bn2.running_mean: torch.Size([768])
encoder.model.blocks.3.4.bn2.running_var: torch.Size([768])
encoder.model.blocks.3.4.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.3.4.se.conv_reduce.weight: torch.Size([48, 768, 1, 1])
encoder.model.blocks.3.4.se.conv_reduce.bias: torch.Size([48])
encoder.model.blocks.3.4.se.conv_expand.weight: torch.Size([768, 48, 1, 1])
encoder.model.blocks.3.4.se.conv_expand.bias: torch.Size([768])
encoder.model.blocks.3.4.conv_pwl.weight: torch.Size([192, 768, 1, 1])
encoder.model.blocks.3.4.bn3.weight: torch.Size([192])
encoder.model.blocks.3.4.bn3.bias: torch.Size([192])
encoder.model.blocks.3.4.bn3.running_mean: torch.Size([192])
encoder.model.blocks.3.4.bn3.running_var: torch.Size([192])
encoder.model.blocks.3.4.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.3.5.conv_pw.weight: torch.Size([768, 192, 1, 1])
encoder.model.blocks.3.5.bn1.weight: torch.Size([768])
encoder.model.blocks.3.5.bn1.bias: torch.Size([768])
encoder.model.blocks.3.5.bn1.running_mean: torch.Size([768])
encoder.model.blocks.3.5.bn1.running_var: torch.Size([768])
encoder.model.blocks.3.5.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.3.5.conv_dw.weight: torch.Size([768, 1, 3, 3])
encoder.model.blocks.3.5.bn2.weight: torch.Size([768])
encoder.model.blocks.3.5.bn2.bias: torch.Size([768])
encoder.model.blocks.3.5.bn2.running_mean: torch.Size([768])
encoder.model.blocks.3.5.bn2.running_var: torch.Size([768])
encoder.model.blocks.3.5.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.3.5.se.conv_reduce.weight: torch.Size([48, 768, 1, 1])
encoder.model.blocks.3.5.se.conv_reduce.bias: torch.Size([48])
encoder.model.blocks.3.5.se.conv_expand.weight: torch.Size([768, 48, 1, 1])
encoder.model.blocks.3.5.se.conv_expand.bias: torch.Size([768])
encoder.model.blocks.3.5.conv_pwl.weight: torch.Size([192, 768, 1, 1])
encoder.model.blocks.3.5.bn3.weight: torch.Size([192])
encoder.model.blocks.3.5.bn3.bias: torch.Size([192])
encoder.model.blocks.3.5.bn3.running_mean: torch.Size([192])
encoder.model.blocks.3.5.bn3.running_var: torch.Size([192])
encoder.model.blocks.3.5.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.3.6.conv_pw.weight: torch.Size([768, 192, 1, 1])
encoder.model.blocks.3.6.bn1.weight: torch.Size([768])
encoder.model.blocks.3.6.bn1.bias: torch.Size([768])
encoder.model.blocks.3.6.bn1.running_mean: torch.Size([768])
encoder.model.blocks.3.6.bn1.running_var: torch.Size([768])
encoder.model.blocks.3.6.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.3.6.conv_dw.weight: torch.Size([768, 1, 3, 3])
encoder.model.blocks.3.6.bn2.weight: torch.Size([768])
encoder.model.blocks.3.6.bn2.bias: torch.Size([768])
encoder.model.blocks.3.6.bn2.running_mean: torch.Size([768])
encoder.model.blocks.3.6.bn2.running_var: torch.Size([768])
encoder.model.blocks.3.6.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.3.6.se.conv_reduce.weight: torch.Size([48, 768, 1, 1])
encoder.model.blocks.3.6.se.conv_reduce.bias: torch.Size([48])
encoder.model.blocks.3.6.se.conv_expand.weight: torch.Size([768, 48, 1, 1])
encoder.model.blocks.3.6.se.conv_expand.bias: torch.Size([768])
encoder.model.blocks.3.6.conv_pwl.weight: torch.Size([192, 768, 1, 1])
encoder.model.blocks.3.6.bn3.weight: torch.Size([192])
encoder.model.blocks.3.6.bn3.bias: torch.Size([192])
encoder.model.blocks.3.6.bn3.running_mean: torch.Size([192])
encoder.model.blocks.3.6.bn3.running_var: torch.Size([192])
encoder.model.blocks.3.6.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.3.7.conv_pw.weight: torch.Size([768, 192, 1, 1])
encoder.model.blocks.3.7.bn1.weight: torch.Size([768])
encoder.model.blocks.3.7.bn1.bias: torch.Size([768])
encoder.model.blocks.3.7.bn1.running_mean: torch.Size([768])
encoder.model.blocks.3.7.bn1.running_var: torch.Size([768])
encoder.model.blocks.3.7.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.3.7.conv_dw.weight: torch.Size([768, 1, 3, 3])
encoder.model.blocks.3.7.bn2.weight: torch.Size([768])
encoder.model.blocks.3.7.bn2.bias: torch.Size([768])
encoder.model.blocks.3.7.bn2.running_mean: torch.Size([768])
encoder.model.blocks.3.7.bn2.running_var: torch.Size([768])
encoder.model.blocks.3.7.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.3.7.se.conv_reduce.weight: torch.Size([48, 768, 1, 1])
encoder.model.blocks.3.7.se.conv_reduce.bias: torch.Size([48])
encoder.model.blocks.3.7.se.conv_expand.weight: torch.Size([768, 48, 1, 1])
encoder.model.blocks.3.7.se.conv_expand.bias: torch.Size([768])
encoder.model.blocks.3.7.conv_pwl.weight: torch.Size([192, 768, 1, 1])
encoder.model.blocks.3.7.bn3.weight: torch.Size([192])
encoder.model.blocks.3.7.bn3.bias: torch.Size([192])
encoder.model.blocks.3.7.bn3.running_mean: torch.Size([192])
encoder.model.blocks.3.7.bn3.running_var: torch.Size([192])
encoder.model.blocks.3.7.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.3.8.conv_pw.weight: torch.Size([768, 192, 1, 1])
encoder.model.blocks.3.8.bn1.weight: torch.Size([768])
encoder.model.blocks.3.8.bn1.bias: torch.Size([768])
encoder.model.blocks.3.8.bn1.running_mean: torch.Size([768])
encoder.model.blocks.3.8.bn1.running_var: torch.Size([768])
encoder.model.blocks.3.8.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.3.8.conv_dw.weight: torch.Size([768, 1, 3, 3])
encoder.model.blocks.3.8.bn2.weight: torch.Size([768])
encoder.model.blocks.3.8.bn2.bias: torch.Size([768])
encoder.model.blocks.3.8.bn2.running_mean: torch.Size([768])
encoder.model.blocks.3.8.bn2.running_var: torch.Size([768])
encoder.model.blocks.3.8.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.3.8.se.conv_reduce.weight: torch.Size([48, 768, 1, 1])
encoder.model.blocks.3.8.se.conv_reduce.bias: torch.Size([48])
encoder.model.blocks.3.8.se.conv_expand.weight: torch.Size([768, 48, 1, 1])
encoder.model.blocks.3.8.se.conv_expand.bias: torch.Size([768])
encoder.model.blocks.3.8.conv_pwl.weight: torch.Size([192, 768, 1, 1])
encoder.model.blocks.3.8.bn3.weight: torch.Size([192])
encoder.model.blocks.3.8.bn3.bias: torch.Size([192])
encoder.model.blocks.3.8.bn3.running_mean: torch.Size([192])
encoder.model.blocks.3.8.bn3.running_var: torch.Size([192])
encoder.model.blocks.3.8.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.3.9.conv_pw.weight: torch.Size([768, 192, 1, 1])
encoder.model.blocks.3.9.bn1.weight: torch.Size([768])
encoder.model.blocks.3.9.bn1.bias: torch.Size([768])
encoder.model.blocks.3.9.bn1.running_mean: torch.Size([768])
encoder.model.blocks.3.9.bn1.running_var: torch.Size([768])
encoder.model.blocks.3.9.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.3.9.conv_dw.weight: torch.Size([768, 1, 3, 3])
encoder.model.blocks.3.9.bn2.weight: torch.Size([768])
encoder.model.blocks.3.9.bn2.bias: torch.Size([768])
encoder.model.blocks.3.9.bn2.running_mean: torch.Size([768])
encoder.model.blocks.3.9.bn2.running_var: torch.Size([768])
encoder.model.blocks.3.9.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.3.9.se.conv_reduce.weight: torch.Size([48, 768, 1, 1])
encoder.model.blocks.3.9.se.conv_reduce.bias: torch.Size([48])
encoder.model.blocks.3.9.se.conv_expand.weight: torch.Size([768, 48, 1, 1])
encoder.model.blocks.3.9.se.conv_expand.bias: torch.Size([768])
encoder.model.blocks.3.9.conv_pwl.weight: torch.Size([192, 768, 1, 1])
encoder.model.blocks.3.9.bn3.weight: torch.Size([192])
encoder.model.blocks.3.9.bn3.bias: torch.Size([192])
encoder.model.blocks.3.9.bn3.running_mean: torch.Size([192])
encoder.model.blocks.3.9.bn3.running_var: torch.Size([192])
encoder.model.blocks.3.9.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.0.conv_pw.weight: torch.Size([1152, 192, 1, 1])
encoder.model.blocks.4.0.bn1.weight: torch.Size([1152])
encoder.model.blocks.4.0.bn1.bias: torch.Size([1152])
encoder.model.blocks.4.0.bn1.running_mean: torch.Size([1152])
encoder.model.blocks.4.0.bn1.running_var: torch.Size([1152])
encoder.model.blocks.4.0.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.0.conv_dw.weight: torch.Size([1152, 1, 3, 3])
encoder.model.blocks.4.0.bn2.weight: torch.Size([1152])
encoder.model.blocks.4.0.bn2.bias: torch.Size([1152])
encoder.model.blocks.4.0.bn2.running_mean: torch.Size([1152])
encoder.model.blocks.4.0.bn2.running_var: torch.Size([1152])
encoder.model.blocks.4.0.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.0.se.conv_reduce.weight: torch.Size([48, 1152, 1, 1])
encoder.model.blocks.4.0.se.conv_reduce.bias: torch.Size([48])
encoder.model.blocks.4.0.se.conv_expand.weight: torch.Size([1152, 48, 1, 1])
encoder.model.blocks.4.0.se.conv_expand.bias: torch.Size([1152])
encoder.model.blocks.4.0.conv_pwl.weight: torch.Size([224, 1152, 1, 1])
encoder.model.blocks.4.0.bn3.weight: torch.Size([224])
encoder.model.blocks.4.0.bn3.bias: torch.Size([224])
encoder.model.blocks.4.0.bn3.running_mean: torch.Size([224])
encoder.model.blocks.4.0.bn3.running_var: torch.Size([224])
encoder.model.blocks.4.0.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.1.conv_pw.weight: torch.Size([1344, 224, 1, 1])
encoder.model.blocks.4.1.bn1.weight: torch.Size([1344])
encoder.model.blocks.4.1.bn1.bias: torch.Size([1344])
encoder.model.blocks.4.1.bn1.running_mean: torch.Size([1344])
encoder.model.blocks.4.1.bn1.running_var: torch.Size([1344])
encoder.model.blocks.4.1.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.1.conv_dw.weight: torch.Size([1344, 1, 3, 3])
encoder.model.blocks.4.1.bn2.weight: torch.Size([1344])
encoder.model.blocks.4.1.bn2.bias: torch.Size([1344])
encoder.model.blocks.4.1.bn2.running_mean: torch.Size([1344])
encoder.model.blocks.4.1.bn2.running_var: torch.Size([1344])
encoder.model.blocks.4.1.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.1.se.conv_reduce.weight: torch.Size([56, 1344, 1, 1])
encoder.model.blocks.4.1.se.conv_reduce.bias: torch.Size([56])
encoder.model.blocks.4.1.se.conv_expand.weight: torch.Size([1344, 56, 1, 1])
encoder.model.blocks.4.1.se.conv_expand.bias: torch.Size([1344])
encoder.model.blocks.4.1.conv_pwl.weight: torch.Size([224, 1344, 1, 1])
encoder.model.blocks.4.1.bn3.weight: torch.Size([224])
encoder.model.blocks.4.1.bn3.bias: torch.Size([224])
encoder.model.blocks.4.1.bn3.running_mean: torch.Size([224])
encoder.model.blocks.4.1.bn3.running_var: torch.Size([224])
encoder.model.blocks.4.1.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.2.conv_pw.weight: torch.Size([1344, 224, 1, 1])
encoder.model.blocks.4.2.bn1.weight: torch.Size([1344])
encoder.model.blocks.4.2.bn1.bias: torch.Size([1344])
encoder.model.blocks.4.2.bn1.running_mean: torch.Size([1344])
encoder.model.blocks.4.2.bn1.running_var: torch.Size([1344])
encoder.model.blocks.4.2.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.2.conv_dw.weight: torch.Size([1344, 1, 3, 3])
encoder.model.blocks.4.2.bn2.weight: torch.Size([1344])
encoder.model.blocks.4.2.bn2.bias: torch.Size([1344])
encoder.model.blocks.4.2.bn2.running_mean: torch.Size([1344])
encoder.model.blocks.4.2.bn2.running_var: torch.Size([1344])
encoder.model.blocks.4.2.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.2.se.conv_reduce.weight: torch.Size([56, 1344, 1, 1])
encoder.model.blocks.4.2.se.conv_reduce.bias: torch.Size([56])
encoder.model.blocks.4.2.se.conv_expand.weight: torch.Size([1344, 56, 1, 1])
encoder.model.blocks.4.2.se.conv_expand.bias: torch.Size([1344])
encoder.model.blocks.4.2.conv_pwl.weight: torch.Size([224, 1344, 1, 1])
encoder.model.blocks.4.2.bn3.weight: torch.Size([224])
encoder.model.blocks.4.2.bn3.bias: torch.Size([224])
encoder.model.blocks.4.2.bn3.running_mean: torch.Size([224])
encoder.model.blocks.4.2.bn3.running_var: torch.Size([224])
encoder.model.blocks.4.2.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.3.conv_pw.weight: torch.Size([1344, 224, 1, 1])
encoder.model.blocks.4.3.bn1.weight: torch.Size([1344])
encoder.model.blocks.4.3.bn1.bias: torch.Size([1344])
encoder.model.blocks.4.3.bn1.running_mean: torch.Size([1344])
encoder.model.blocks.4.3.bn1.running_var: torch.Size([1344])
encoder.model.blocks.4.3.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.3.conv_dw.weight: torch.Size([1344, 1, 3, 3])
encoder.model.blocks.4.3.bn2.weight: torch.Size([1344])
encoder.model.blocks.4.3.bn2.bias: torch.Size([1344])
encoder.model.blocks.4.3.bn2.running_mean: torch.Size([1344])
encoder.model.blocks.4.3.bn2.running_var: torch.Size([1344])
encoder.model.blocks.4.3.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.3.se.conv_reduce.weight: torch.Size([56, 1344, 1, 1])
encoder.model.blocks.4.3.se.conv_reduce.bias: torch.Size([56])
encoder.model.blocks.4.3.se.conv_expand.weight: torch.Size([1344, 56, 1, 1])
encoder.model.blocks.4.3.se.conv_expand.bias: torch.Size([1344])
encoder.model.blocks.4.3.conv_pwl.weight: torch.Size([224, 1344, 1, 1])
encoder.model.blocks.4.3.bn3.weight: torch.Size([224])
encoder.model.blocks.4.3.bn3.bias: torch.Size([224])
encoder.model.blocks.4.3.bn3.running_mean: torch.Size([224])
encoder.model.blocks.4.3.bn3.running_var: torch.Size([224])
encoder.model.blocks.4.3.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.4.conv_pw.weight: torch.Size([1344, 224, 1, 1])
encoder.model.blocks.4.4.bn1.weight: torch.Size([1344])
encoder.model.blocks.4.4.bn1.bias: torch.Size([1344])
encoder.model.blocks.4.4.bn1.running_mean: torch.Size([1344])
encoder.model.blocks.4.4.bn1.running_var: torch.Size([1344])
encoder.model.blocks.4.4.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.4.conv_dw.weight: torch.Size([1344, 1, 3, 3])
encoder.model.blocks.4.4.bn2.weight: torch.Size([1344])
encoder.model.blocks.4.4.bn2.bias: torch.Size([1344])
encoder.model.blocks.4.4.bn2.running_mean: torch.Size([1344])
encoder.model.blocks.4.4.bn2.running_var: torch.Size([1344])
encoder.model.blocks.4.4.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.4.se.conv_reduce.weight: torch.Size([56, 1344, 1, 1])
encoder.model.blocks.4.4.se.conv_reduce.bias: torch.Size([56])
encoder.model.blocks.4.4.se.conv_expand.weight: torch.Size([1344, 56, 1, 1])
encoder.model.blocks.4.4.se.conv_expand.bias: torch.Size([1344])
encoder.model.blocks.4.4.conv_pwl.weight: torch.Size([224, 1344, 1, 1])
encoder.model.blocks.4.4.bn3.weight: torch.Size([224])
encoder.model.blocks.4.4.bn3.bias: torch.Size([224])
encoder.model.blocks.4.4.bn3.running_mean: torch.Size([224])
encoder.model.blocks.4.4.bn3.running_var: torch.Size([224])
encoder.model.blocks.4.4.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.5.conv_pw.weight: torch.Size([1344, 224, 1, 1])
encoder.model.blocks.4.5.bn1.weight: torch.Size([1344])
encoder.model.blocks.4.5.bn1.bias: torch.Size([1344])
encoder.model.blocks.4.5.bn1.running_mean: torch.Size([1344])
encoder.model.blocks.4.5.bn1.running_var: torch.Size([1344])
encoder.model.blocks.4.5.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.5.conv_dw.weight: torch.Size([1344, 1, 3, 3])
encoder.model.blocks.4.5.bn2.weight: torch.Size([1344])
encoder.model.blocks.4.5.bn2.bias: torch.Size([1344])
encoder.model.blocks.4.5.bn2.running_mean: torch.Size([1344])
encoder.model.blocks.4.5.bn2.running_var: torch.Size([1344])
encoder.model.blocks.4.5.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.5.se.conv_reduce.weight: torch.Size([56, 1344, 1, 1])
encoder.model.blocks.4.5.se.conv_reduce.bias: torch.Size([56])
encoder.model.blocks.4.5.se.conv_expand.weight: torch.Size([1344, 56, 1, 1])
encoder.model.blocks.4.5.se.conv_expand.bias: torch.Size([1344])
encoder.model.blocks.4.5.conv_pwl.weight: torch.Size([224, 1344, 1, 1])
encoder.model.blocks.4.5.bn3.weight: torch.Size([224])
encoder.model.blocks.4.5.bn3.bias: torch.Size([224])
encoder.model.blocks.4.5.bn3.running_mean: torch.Size([224])
encoder.model.blocks.4.5.bn3.running_var: torch.Size([224])
encoder.model.blocks.4.5.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.6.conv_pw.weight: torch.Size([1344, 224, 1, 1])
encoder.model.blocks.4.6.bn1.weight: torch.Size([1344])
encoder.model.blocks.4.6.bn1.bias: torch.Size([1344])
encoder.model.blocks.4.6.bn1.running_mean: torch.Size([1344])
encoder.model.blocks.4.6.bn1.running_var: torch.Size([1344])
encoder.model.blocks.4.6.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.6.conv_dw.weight: torch.Size([1344, 1, 3, 3])
encoder.model.blocks.4.6.bn2.weight: torch.Size([1344])
encoder.model.blocks.4.6.bn2.bias: torch.Size([1344])
encoder.model.blocks.4.6.bn2.running_mean: torch.Size([1344])
encoder.model.blocks.4.6.bn2.running_var: torch.Size([1344])
encoder.model.blocks.4.6.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.6.se.conv_reduce.weight: torch.Size([56, 1344, 1, 1])
encoder.model.blocks.4.6.se.conv_reduce.bias: torch.Size([56])
encoder.model.blocks.4.6.se.conv_expand.weight: torch.Size([1344, 56, 1, 1])
encoder.model.blocks.4.6.se.conv_expand.bias: torch.Size([1344])
encoder.model.blocks.4.6.conv_pwl.weight: torch.Size([224, 1344, 1, 1])
encoder.model.blocks.4.6.bn3.weight: torch.Size([224])
encoder.model.blocks.4.6.bn3.bias: torch.Size([224])
encoder.model.blocks.4.6.bn3.running_mean: torch.Size([224])
encoder.model.blocks.4.6.bn3.running_var: torch.Size([224])
encoder.model.blocks.4.6.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.7.conv_pw.weight: torch.Size([1344, 224, 1, 1])
encoder.model.blocks.4.7.bn1.weight: torch.Size([1344])
encoder.model.blocks.4.7.bn1.bias: torch.Size([1344])
encoder.model.blocks.4.7.bn1.running_mean: torch.Size([1344])
encoder.model.blocks.4.7.bn1.running_var: torch.Size([1344])
encoder.model.blocks.4.7.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.7.conv_dw.weight: torch.Size([1344, 1, 3, 3])
encoder.model.blocks.4.7.bn2.weight: torch.Size([1344])
encoder.model.blocks.4.7.bn2.bias: torch.Size([1344])
encoder.model.blocks.4.7.bn2.running_mean: torch.Size([1344])
encoder.model.blocks.4.7.bn2.running_var: torch.Size([1344])
encoder.model.blocks.4.7.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.7.se.conv_reduce.weight: torch.Size([56, 1344, 1, 1])
encoder.model.blocks.4.7.se.conv_reduce.bias: torch.Size([56])
encoder.model.blocks.4.7.se.conv_expand.weight: torch.Size([1344, 56, 1, 1])
encoder.model.blocks.4.7.se.conv_expand.bias: torch.Size([1344])
encoder.model.blocks.4.7.conv_pwl.weight: torch.Size([224, 1344, 1, 1])
encoder.model.blocks.4.7.bn3.weight: torch.Size([224])
encoder.model.blocks.4.7.bn3.bias: torch.Size([224])
encoder.model.blocks.4.7.bn3.running_mean: torch.Size([224])
encoder.model.blocks.4.7.bn3.running_var: torch.Size([224])
encoder.model.blocks.4.7.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.8.conv_pw.weight: torch.Size([1344, 224, 1, 1])
encoder.model.blocks.4.8.bn1.weight: torch.Size([1344])
encoder.model.blocks.4.8.bn1.bias: torch.Size([1344])
encoder.model.blocks.4.8.bn1.running_mean: torch.Size([1344])
encoder.model.blocks.4.8.bn1.running_var: torch.Size([1344])
encoder.model.blocks.4.8.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.8.conv_dw.weight: torch.Size([1344, 1, 3, 3])
encoder.model.blocks.4.8.bn2.weight: torch.Size([1344])
encoder.model.blocks.4.8.bn2.bias: torch.Size([1344])
encoder.model.blocks.4.8.bn2.running_mean: torch.Size([1344])
encoder.model.blocks.4.8.bn2.running_var: torch.Size([1344])
encoder.model.blocks.4.8.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.8.se.conv_reduce.weight: torch.Size([56, 1344, 1, 1])
encoder.model.blocks.4.8.se.conv_reduce.bias: torch.Size([56])
encoder.model.blocks.4.8.se.conv_expand.weight: torch.Size([1344, 56, 1, 1])
encoder.model.blocks.4.8.se.conv_expand.bias: torch.Size([1344])
encoder.model.blocks.4.8.conv_pwl.weight: torch.Size([224, 1344, 1, 1])
encoder.model.blocks.4.8.bn3.weight: torch.Size([224])
encoder.model.blocks.4.8.bn3.bias: torch.Size([224])
encoder.model.blocks.4.8.bn3.running_mean: torch.Size([224])
encoder.model.blocks.4.8.bn3.running_var: torch.Size([224])
encoder.model.blocks.4.8.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.9.conv_pw.weight: torch.Size([1344, 224, 1, 1])
encoder.model.blocks.4.9.bn1.weight: torch.Size([1344])
encoder.model.blocks.4.9.bn1.bias: torch.Size([1344])
encoder.model.blocks.4.9.bn1.running_mean: torch.Size([1344])
encoder.model.blocks.4.9.bn1.running_var: torch.Size([1344])
encoder.model.blocks.4.9.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.9.conv_dw.weight: torch.Size([1344, 1, 3, 3])
encoder.model.blocks.4.9.bn2.weight: torch.Size([1344])
encoder.model.blocks.4.9.bn2.bias: torch.Size([1344])
encoder.model.blocks.4.9.bn2.running_mean: torch.Size([1344])
encoder.model.blocks.4.9.bn2.running_var: torch.Size([1344])
encoder.model.blocks.4.9.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.9.se.conv_reduce.weight: torch.Size([56, 1344, 1, 1])
encoder.model.blocks.4.9.se.conv_reduce.bias: torch.Size([56])
encoder.model.blocks.4.9.se.conv_expand.weight: torch.Size([1344, 56, 1, 1])
encoder.model.blocks.4.9.se.conv_expand.bias: torch.Size([1344])
encoder.model.blocks.4.9.conv_pwl.weight: torch.Size([224, 1344, 1, 1])
encoder.model.blocks.4.9.bn3.weight: torch.Size([224])
encoder.model.blocks.4.9.bn3.bias: torch.Size([224])
encoder.model.blocks.4.9.bn3.running_mean: torch.Size([224])
encoder.model.blocks.4.9.bn3.running_var: torch.Size([224])
encoder.model.blocks.4.9.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.10.conv_pw.weight: torch.Size([1344, 224, 1, 1])
encoder.model.blocks.4.10.bn1.weight: torch.Size([1344])
encoder.model.blocks.4.10.bn1.bias: torch.Size([1344])
encoder.model.blocks.4.10.bn1.running_mean: torch.Size([1344])
encoder.model.blocks.4.10.bn1.running_var: torch.Size([1344])
encoder.model.blocks.4.10.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.10.conv_dw.weight: torch.Size([1344, 1, 3, 3])
encoder.model.blocks.4.10.bn2.weight: torch.Size([1344])
encoder.model.blocks.4.10.bn2.bias: torch.Size([1344])
encoder.model.blocks.4.10.bn2.running_mean: torch.Size([1344])
encoder.model.blocks.4.10.bn2.running_var: torch.Size([1344])
encoder.model.blocks.4.10.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.10.se.conv_reduce.weight: torch.Size([56, 1344, 1, 1])
encoder.model.blocks.4.10.se.conv_reduce.bias: torch.Size([56])
encoder.model.blocks.4.10.se.conv_expand.weight: torch.Size([1344, 56, 1, 1])
encoder.model.blocks.4.10.se.conv_expand.bias: torch.Size([1344])
encoder.model.blocks.4.10.conv_pwl.weight: torch.Size([224, 1344, 1, 1])
encoder.model.blocks.4.10.bn3.weight: torch.Size([224])
encoder.model.blocks.4.10.bn3.bias: torch.Size([224])
encoder.model.blocks.4.10.bn3.running_mean: torch.Size([224])
encoder.model.blocks.4.10.bn3.running_var: torch.Size([224])
encoder.model.blocks.4.10.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.11.conv_pw.weight: torch.Size([1344, 224, 1, 1])
encoder.model.blocks.4.11.bn1.weight: torch.Size([1344])
encoder.model.blocks.4.11.bn1.bias: torch.Size([1344])
encoder.model.blocks.4.11.bn1.running_mean: torch.Size([1344])
encoder.model.blocks.4.11.bn1.running_var: torch.Size([1344])
encoder.model.blocks.4.11.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.11.conv_dw.weight: torch.Size([1344, 1, 3, 3])
encoder.model.blocks.4.11.bn2.weight: torch.Size([1344])
encoder.model.blocks.4.11.bn2.bias: torch.Size([1344])
encoder.model.blocks.4.11.bn2.running_mean: torch.Size([1344])
encoder.model.blocks.4.11.bn2.running_var: torch.Size([1344])
encoder.model.blocks.4.11.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.11.se.conv_reduce.weight: torch.Size([56, 1344, 1, 1])
encoder.model.blocks.4.11.se.conv_reduce.bias: torch.Size([56])
encoder.model.blocks.4.11.se.conv_expand.weight: torch.Size([1344, 56, 1, 1])
encoder.model.blocks.4.11.se.conv_expand.bias: torch.Size([1344])
encoder.model.blocks.4.11.conv_pwl.weight: torch.Size([224, 1344, 1, 1])
encoder.model.blocks.4.11.bn3.weight: torch.Size([224])
encoder.model.blocks.4.11.bn3.bias: torch.Size([224])
encoder.model.blocks.4.11.bn3.running_mean: torch.Size([224])
encoder.model.blocks.4.11.bn3.running_var: torch.Size([224])
encoder.model.blocks.4.11.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.12.conv_pw.weight: torch.Size([1344, 224, 1, 1])
encoder.model.blocks.4.12.bn1.weight: torch.Size([1344])
encoder.model.blocks.4.12.bn1.bias: torch.Size([1344])
encoder.model.blocks.4.12.bn1.running_mean: torch.Size([1344])
encoder.model.blocks.4.12.bn1.running_var: torch.Size([1344])
encoder.model.blocks.4.12.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.12.conv_dw.weight: torch.Size([1344, 1, 3, 3])
encoder.model.blocks.4.12.bn2.weight: torch.Size([1344])
encoder.model.blocks.4.12.bn2.bias: torch.Size([1344])
encoder.model.blocks.4.12.bn2.running_mean: torch.Size([1344])
encoder.model.blocks.4.12.bn2.running_var: torch.Size([1344])
encoder.model.blocks.4.12.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.12.se.conv_reduce.weight: torch.Size([56, 1344, 1, 1])
encoder.model.blocks.4.12.se.conv_reduce.bias: torch.Size([56])
encoder.model.blocks.4.12.se.conv_expand.weight: torch.Size([1344, 56, 1, 1])
encoder.model.blocks.4.12.se.conv_expand.bias: torch.Size([1344])
encoder.model.blocks.4.12.conv_pwl.weight: torch.Size([224, 1344, 1, 1])
encoder.model.blocks.4.12.bn3.weight: torch.Size([224])
encoder.model.blocks.4.12.bn3.bias: torch.Size([224])
encoder.model.blocks.4.12.bn3.running_mean: torch.Size([224])
encoder.model.blocks.4.12.bn3.running_var: torch.Size([224])
encoder.model.blocks.4.12.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.13.conv_pw.weight: torch.Size([1344, 224, 1, 1])
encoder.model.blocks.4.13.bn1.weight: torch.Size([1344])
encoder.model.blocks.4.13.bn1.bias: torch.Size([1344])
encoder.model.blocks.4.13.bn1.running_mean: torch.Size([1344])
encoder.model.blocks.4.13.bn1.running_var: torch.Size([1344])
encoder.model.blocks.4.13.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.13.conv_dw.weight: torch.Size([1344, 1, 3, 3])
encoder.model.blocks.4.13.bn2.weight: torch.Size([1344])
encoder.model.blocks.4.13.bn2.bias: torch.Size([1344])
encoder.model.blocks.4.13.bn2.running_mean: torch.Size([1344])
encoder.model.blocks.4.13.bn2.running_var: torch.Size([1344])
encoder.model.blocks.4.13.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.13.se.conv_reduce.weight: torch.Size([56, 1344, 1, 1])
encoder.model.blocks.4.13.se.conv_reduce.bias: torch.Size([56])
encoder.model.blocks.4.13.se.conv_expand.weight: torch.Size([1344, 56, 1, 1])
encoder.model.blocks.4.13.se.conv_expand.bias: torch.Size([1344])
encoder.model.blocks.4.13.conv_pwl.weight: torch.Size([224, 1344, 1, 1])
encoder.model.blocks.4.13.bn3.weight: torch.Size([224])
encoder.model.blocks.4.13.bn3.bias: torch.Size([224])
encoder.model.blocks.4.13.bn3.running_mean: torch.Size([224])
encoder.model.blocks.4.13.bn3.running_var: torch.Size([224])
encoder.model.blocks.4.13.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.14.conv_pw.weight: torch.Size([1344, 224, 1, 1])
encoder.model.blocks.4.14.bn1.weight: torch.Size([1344])
encoder.model.blocks.4.14.bn1.bias: torch.Size([1344])
encoder.model.blocks.4.14.bn1.running_mean: torch.Size([1344])
encoder.model.blocks.4.14.bn1.running_var: torch.Size([1344])
encoder.model.blocks.4.14.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.14.conv_dw.weight: torch.Size([1344, 1, 3, 3])
encoder.model.blocks.4.14.bn2.weight: torch.Size([1344])
encoder.model.blocks.4.14.bn2.bias: torch.Size([1344])
encoder.model.blocks.4.14.bn2.running_mean: torch.Size([1344])
encoder.model.blocks.4.14.bn2.running_var: torch.Size([1344])
encoder.model.blocks.4.14.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.14.se.conv_reduce.weight: torch.Size([56, 1344, 1, 1])
encoder.model.blocks.4.14.se.conv_reduce.bias: torch.Size([56])
encoder.model.blocks.4.14.se.conv_expand.weight: torch.Size([1344, 56, 1, 1])
encoder.model.blocks.4.14.se.conv_expand.bias: torch.Size([1344])
encoder.model.blocks.4.14.conv_pwl.weight: torch.Size([224, 1344, 1, 1])
encoder.model.blocks.4.14.bn3.weight: torch.Size([224])
encoder.model.blocks.4.14.bn3.bias: torch.Size([224])
encoder.model.blocks.4.14.bn3.running_mean: torch.Size([224])
encoder.model.blocks.4.14.bn3.running_var: torch.Size([224])
encoder.model.blocks.4.14.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.15.conv_pw.weight: torch.Size([1344, 224, 1, 1])
encoder.model.blocks.4.15.bn1.weight: torch.Size([1344])
encoder.model.blocks.4.15.bn1.bias: torch.Size([1344])
encoder.model.blocks.4.15.bn1.running_mean: torch.Size([1344])
encoder.model.blocks.4.15.bn1.running_var: torch.Size([1344])
encoder.model.blocks.4.15.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.15.conv_dw.weight: torch.Size([1344, 1, 3, 3])
encoder.model.blocks.4.15.bn2.weight: torch.Size([1344])
encoder.model.blocks.4.15.bn2.bias: torch.Size([1344])
encoder.model.blocks.4.15.bn2.running_mean: torch.Size([1344])
encoder.model.blocks.4.15.bn2.running_var: torch.Size([1344])
encoder.model.blocks.4.15.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.15.se.conv_reduce.weight: torch.Size([56, 1344, 1, 1])
encoder.model.blocks.4.15.se.conv_reduce.bias: torch.Size([56])
encoder.model.blocks.4.15.se.conv_expand.weight: torch.Size([1344, 56, 1, 1])
encoder.model.blocks.4.15.se.conv_expand.bias: torch.Size([1344])
encoder.model.blocks.4.15.conv_pwl.weight: torch.Size([224, 1344, 1, 1])
encoder.model.blocks.4.15.bn3.weight: torch.Size([224])
encoder.model.blocks.4.15.bn3.bias: torch.Size([224])
encoder.model.blocks.4.15.bn3.running_mean: torch.Size([224])
encoder.model.blocks.4.15.bn3.running_var: torch.Size([224])
encoder.model.blocks.4.15.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.16.conv_pw.weight: torch.Size([1344, 224, 1, 1])
encoder.model.blocks.4.16.bn1.weight: torch.Size([1344])
encoder.model.blocks.4.16.bn1.bias: torch.Size([1344])
encoder.model.blocks.4.16.bn1.running_mean: torch.Size([1344])
encoder.model.blocks.4.16.bn1.running_var: torch.Size([1344])
encoder.model.blocks.4.16.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.16.conv_dw.weight: torch.Size([1344, 1, 3, 3])
encoder.model.blocks.4.16.bn2.weight: torch.Size([1344])
encoder.model.blocks.4.16.bn2.bias: torch.Size([1344])
encoder.model.blocks.4.16.bn2.running_mean: torch.Size([1344])
encoder.model.blocks.4.16.bn2.running_var: torch.Size([1344])
encoder.model.blocks.4.16.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.16.se.conv_reduce.weight: torch.Size([56, 1344, 1, 1])
encoder.model.blocks.4.16.se.conv_reduce.bias: torch.Size([56])
encoder.model.blocks.4.16.se.conv_expand.weight: torch.Size([1344, 56, 1, 1])
encoder.model.blocks.4.16.se.conv_expand.bias: torch.Size([1344])
encoder.model.blocks.4.16.conv_pwl.weight: torch.Size([224, 1344, 1, 1])
encoder.model.blocks.4.16.bn3.weight: torch.Size([224])
encoder.model.blocks.4.16.bn3.bias: torch.Size([224])
encoder.model.blocks.4.16.bn3.running_mean: torch.Size([224])
encoder.model.blocks.4.16.bn3.running_var: torch.Size([224])
encoder.model.blocks.4.16.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.17.conv_pw.weight: torch.Size([1344, 224, 1, 1])
encoder.model.blocks.4.17.bn1.weight: torch.Size([1344])
encoder.model.blocks.4.17.bn1.bias: torch.Size([1344])
encoder.model.blocks.4.17.bn1.running_mean: torch.Size([1344])
encoder.model.blocks.4.17.bn1.running_var: torch.Size([1344])
encoder.model.blocks.4.17.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.17.conv_dw.weight: torch.Size([1344, 1, 3, 3])
encoder.model.blocks.4.17.bn2.weight: torch.Size([1344])
encoder.model.blocks.4.17.bn2.bias: torch.Size([1344])
encoder.model.blocks.4.17.bn2.running_mean: torch.Size([1344])
encoder.model.blocks.4.17.bn2.running_var: torch.Size([1344])
encoder.model.blocks.4.17.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.17.se.conv_reduce.weight: torch.Size([56, 1344, 1, 1])
encoder.model.blocks.4.17.se.conv_reduce.bias: torch.Size([56])
encoder.model.blocks.4.17.se.conv_expand.weight: torch.Size([1344, 56, 1, 1])
encoder.model.blocks.4.17.se.conv_expand.bias: torch.Size([1344])
encoder.model.blocks.4.17.conv_pwl.weight: torch.Size([224, 1344, 1, 1])
encoder.model.blocks.4.17.bn3.weight: torch.Size([224])
encoder.model.blocks.4.17.bn3.bias: torch.Size([224])
encoder.model.blocks.4.17.bn3.running_mean: torch.Size([224])
encoder.model.blocks.4.17.bn3.running_var: torch.Size([224])
encoder.model.blocks.4.17.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.18.conv_pw.weight: torch.Size([1344, 224, 1, 1])
encoder.model.blocks.4.18.bn1.weight: torch.Size([1344])
encoder.model.blocks.4.18.bn1.bias: torch.Size([1344])
encoder.model.blocks.4.18.bn1.running_mean: torch.Size([1344])
encoder.model.blocks.4.18.bn1.running_var: torch.Size([1344])
encoder.model.blocks.4.18.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.18.conv_dw.weight: torch.Size([1344, 1, 3, 3])
encoder.model.blocks.4.18.bn2.weight: torch.Size([1344])
encoder.model.blocks.4.18.bn2.bias: torch.Size([1344])
encoder.model.blocks.4.18.bn2.running_mean: torch.Size([1344])
encoder.model.blocks.4.18.bn2.running_var: torch.Size([1344])
encoder.model.blocks.4.18.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.4.18.se.conv_reduce.weight: torch.Size([56, 1344, 1, 1])
encoder.model.blocks.4.18.se.conv_reduce.bias: torch.Size([56])
encoder.model.blocks.4.18.se.conv_expand.weight: torch.Size([1344, 56, 1, 1])
encoder.model.blocks.4.18.se.conv_expand.bias: torch.Size([1344])
encoder.model.blocks.4.18.conv_pwl.weight: torch.Size([224, 1344, 1, 1])
encoder.model.blocks.4.18.bn3.weight: torch.Size([224])
encoder.model.blocks.4.18.bn3.bias: torch.Size([224])
encoder.model.blocks.4.18.bn3.running_mean: torch.Size([224])
encoder.model.blocks.4.18.bn3.running_var: torch.Size([224])
encoder.model.blocks.4.18.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.0.conv_pw.weight: torch.Size([1344, 224, 1, 1])
encoder.model.blocks.5.0.bn1.weight: torch.Size([1344])
encoder.model.blocks.5.0.bn1.bias: torch.Size([1344])
encoder.model.blocks.5.0.bn1.running_mean: torch.Size([1344])
encoder.model.blocks.5.0.bn1.running_var: torch.Size([1344])
encoder.model.blocks.5.0.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.0.conv_dw.weight: torch.Size([1344, 1, 3, 3])
encoder.model.blocks.5.0.bn2.weight: torch.Size([1344])
encoder.model.blocks.5.0.bn2.bias: torch.Size([1344])
encoder.model.blocks.5.0.bn2.running_mean: torch.Size([1344])
encoder.model.blocks.5.0.bn2.running_var: torch.Size([1344])
encoder.model.blocks.5.0.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.0.se.conv_reduce.weight: torch.Size([56, 1344, 1, 1])
encoder.model.blocks.5.0.se.conv_reduce.bias: torch.Size([56])
encoder.model.blocks.5.0.se.conv_expand.weight: torch.Size([1344, 56, 1, 1])
encoder.model.blocks.5.0.se.conv_expand.bias: torch.Size([1344])
encoder.model.blocks.5.0.conv_pwl.weight: torch.Size([384, 1344, 1, 1])
encoder.model.blocks.5.0.bn3.weight: torch.Size([384])
encoder.model.blocks.5.0.bn3.bias: torch.Size([384])
encoder.model.blocks.5.0.bn3.running_mean: torch.Size([384])
encoder.model.blocks.5.0.bn3.running_var: torch.Size([384])
encoder.model.blocks.5.0.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.1.conv_pw.weight: torch.Size([2304, 384, 1, 1])
encoder.model.blocks.5.1.bn1.weight: torch.Size([2304])
encoder.model.blocks.5.1.bn1.bias: torch.Size([2304])
encoder.model.blocks.5.1.bn1.running_mean: torch.Size([2304])
encoder.model.blocks.5.1.bn1.running_var: torch.Size([2304])
encoder.model.blocks.5.1.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.1.conv_dw.weight: torch.Size([2304, 1, 3, 3])
encoder.model.blocks.5.1.bn2.weight: torch.Size([2304])
encoder.model.blocks.5.1.bn2.bias: torch.Size([2304])
encoder.model.blocks.5.1.bn2.running_mean: torch.Size([2304])
encoder.model.blocks.5.1.bn2.running_var: torch.Size([2304])
encoder.model.blocks.5.1.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.1.se.conv_reduce.weight: torch.Size([96, 2304, 1, 1])
encoder.model.blocks.5.1.se.conv_reduce.bias: torch.Size([96])
encoder.model.blocks.5.1.se.conv_expand.weight: torch.Size([2304, 96, 1, 1])
encoder.model.blocks.5.1.se.conv_expand.bias: torch.Size([2304])
encoder.model.blocks.5.1.conv_pwl.weight: torch.Size([384, 2304, 1, 1])
encoder.model.blocks.5.1.bn3.weight: torch.Size([384])
encoder.model.blocks.5.1.bn3.bias: torch.Size([384])
encoder.model.blocks.5.1.bn3.running_mean: torch.Size([384])
encoder.model.blocks.5.1.bn3.running_var: torch.Size([384])
encoder.model.blocks.5.1.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.2.conv_pw.weight: torch.Size([2304, 384, 1, 1])
encoder.model.blocks.5.2.bn1.weight: torch.Size([2304])
encoder.model.blocks.5.2.bn1.bias: torch.Size([2304])
encoder.model.blocks.5.2.bn1.running_mean: torch.Size([2304])
encoder.model.blocks.5.2.bn1.running_var: torch.Size([2304])
encoder.model.blocks.5.2.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.2.conv_dw.weight: torch.Size([2304, 1, 3, 3])
encoder.model.blocks.5.2.bn2.weight: torch.Size([2304])
encoder.model.blocks.5.2.bn2.bias: torch.Size([2304])
encoder.model.blocks.5.2.bn2.running_mean: torch.Size([2304])
encoder.model.blocks.5.2.bn2.running_var: torch.Size([2304])
encoder.model.blocks.5.2.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.2.se.conv_reduce.weight: torch.Size([96, 2304, 1, 1])
encoder.model.blocks.5.2.se.conv_reduce.bias: torch.Size([96])
encoder.model.blocks.5.2.se.conv_expand.weight: torch.Size([2304, 96, 1, 1])
encoder.model.blocks.5.2.se.conv_expand.bias: torch.Size([2304])
encoder.model.blocks.5.2.conv_pwl.weight: torch.Size([384, 2304, 1, 1])
encoder.model.blocks.5.2.bn3.weight: torch.Size([384])
encoder.model.blocks.5.2.bn3.bias: torch.Size([384])
encoder.model.blocks.5.2.bn3.running_mean: torch.Size([384])
encoder.model.blocks.5.2.bn3.running_var: torch.Size([384])
encoder.model.blocks.5.2.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.3.conv_pw.weight: torch.Size([2304, 384, 1, 1])
encoder.model.blocks.5.3.bn1.weight: torch.Size([2304])
encoder.model.blocks.5.3.bn1.bias: torch.Size([2304])
encoder.model.blocks.5.3.bn1.running_mean: torch.Size([2304])
encoder.model.blocks.5.3.bn1.running_var: torch.Size([2304])
encoder.model.blocks.5.3.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.3.conv_dw.weight: torch.Size([2304, 1, 3, 3])
encoder.model.blocks.5.3.bn2.weight: torch.Size([2304])
encoder.model.blocks.5.3.bn2.bias: torch.Size([2304])
encoder.model.blocks.5.3.bn2.running_mean: torch.Size([2304])
encoder.model.blocks.5.3.bn2.running_var: torch.Size([2304])
encoder.model.blocks.5.3.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.3.se.conv_reduce.weight: torch.Size([96, 2304, 1, 1])
encoder.model.blocks.5.3.se.conv_reduce.bias: torch.Size([96])
encoder.model.blocks.5.3.se.conv_expand.weight: torch.Size([2304, 96, 1, 1])
encoder.model.blocks.5.3.se.conv_expand.bias: torch.Size([2304])
encoder.model.blocks.5.3.conv_pwl.weight: torch.Size([384, 2304, 1, 1])
encoder.model.blocks.5.3.bn3.weight: torch.Size([384])
encoder.model.blocks.5.3.bn3.bias: torch.Size([384])
encoder.model.blocks.5.3.bn3.running_mean: torch.Size([384])
encoder.model.blocks.5.3.bn3.running_var: torch.Size([384])
encoder.model.blocks.5.3.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.4.conv_pw.weight: torch.Size([2304, 384, 1, 1])
encoder.model.blocks.5.4.bn1.weight: torch.Size([2304])
encoder.model.blocks.5.4.bn1.bias: torch.Size([2304])
encoder.model.blocks.5.4.bn1.running_mean: torch.Size([2304])
encoder.model.blocks.5.4.bn1.running_var: torch.Size([2304])
encoder.model.blocks.5.4.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.4.conv_dw.weight: torch.Size([2304, 1, 3, 3])
encoder.model.blocks.5.4.bn2.weight: torch.Size([2304])
encoder.model.blocks.5.4.bn2.bias: torch.Size([2304])
encoder.model.blocks.5.4.bn2.running_mean: torch.Size([2304])
encoder.model.blocks.5.4.bn2.running_var: torch.Size([2304])
encoder.model.blocks.5.4.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.4.se.conv_reduce.weight: torch.Size([96, 2304, 1, 1])
encoder.model.blocks.5.4.se.conv_reduce.bias: torch.Size([96])
encoder.model.blocks.5.4.se.conv_expand.weight: torch.Size([2304, 96, 1, 1])
encoder.model.blocks.5.4.se.conv_expand.bias: torch.Size([2304])
encoder.model.blocks.5.4.conv_pwl.weight: torch.Size([384, 2304, 1, 1])
encoder.model.blocks.5.4.bn3.weight: torch.Size([384])
encoder.model.blocks.5.4.bn3.bias: torch.Size([384])
encoder.model.blocks.5.4.bn3.running_mean: torch.Size([384])
encoder.model.blocks.5.4.bn3.running_var: torch.Size([384])
encoder.model.blocks.5.4.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.5.conv_pw.weight: torch.Size([2304, 384, 1, 1])
encoder.model.blocks.5.5.bn1.weight: torch.Size([2304])
encoder.model.blocks.5.5.bn1.bias: torch.Size([2304])
encoder.model.blocks.5.5.bn1.running_mean: torch.Size([2304])
encoder.model.blocks.5.5.bn1.running_var: torch.Size([2304])
encoder.model.blocks.5.5.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.5.conv_dw.weight: torch.Size([2304, 1, 3, 3])
encoder.model.blocks.5.5.bn2.weight: torch.Size([2304])
encoder.model.blocks.5.5.bn2.bias: torch.Size([2304])
encoder.model.blocks.5.5.bn2.running_mean: torch.Size([2304])
encoder.model.blocks.5.5.bn2.running_var: torch.Size([2304])
encoder.model.blocks.5.5.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.5.se.conv_reduce.weight: torch.Size([96, 2304, 1, 1])
encoder.model.blocks.5.5.se.conv_reduce.bias: torch.Size([96])
encoder.model.blocks.5.5.se.conv_expand.weight: torch.Size([2304, 96, 1, 1])
encoder.model.blocks.5.5.se.conv_expand.bias: torch.Size([2304])
encoder.model.blocks.5.5.conv_pwl.weight: torch.Size([384, 2304, 1, 1])
encoder.model.blocks.5.5.bn3.weight: torch.Size([384])
encoder.model.blocks.5.5.bn3.bias: torch.Size([384])
encoder.model.blocks.5.5.bn3.running_mean: torch.Size([384])
encoder.model.blocks.5.5.bn3.running_var: torch.Size([384])
encoder.model.blocks.5.5.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.6.conv_pw.weight: torch.Size([2304, 384, 1, 1])
encoder.model.blocks.5.6.bn1.weight: torch.Size([2304])
encoder.model.blocks.5.6.bn1.bias: torch.Size([2304])
encoder.model.blocks.5.6.bn1.running_mean: torch.Size([2304])
encoder.model.blocks.5.6.bn1.running_var: torch.Size([2304])
encoder.model.blocks.5.6.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.6.conv_dw.weight: torch.Size([2304, 1, 3, 3])
encoder.model.blocks.5.6.bn2.weight: torch.Size([2304])
encoder.model.blocks.5.6.bn2.bias: torch.Size([2304])
encoder.model.blocks.5.6.bn2.running_mean: torch.Size([2304])
encoder.model.blocks.5.6.bn2.running_var: torch.Size([2304])
encoder.model.blocks.5.6.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.6.se.conv_reduce.weight: torch.Size([96, 2304, 1, 1])
encoder.model.blocks.5.6.se.conv_reduce.bias: torch.Size([96])
encoder.model.blocks.5.6.se.conv_expand.weight: torch.Size([2304, 96, 1, 1])
encoder.model.blocks.5.6.se.conv_expand.bias: torch.Size([2304])
encoder.model.blocks.5.6.conv_pwl.weight: torch.Size([384, 2304, 1, 1])
encoder.model.blocks.5.6.bn3.weight: torch.Size([384])
encoder.model.blocks.5.6.bn3.bias: torch.Size([384])
encoder.model.blocks.5.6.bn3.running_mean: torch.Size([384])
encoder.model.blocks.5.6.bn3.running_var: torch.Size([384])
encoder.model.blocks.5.6.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.7.conv_pw.weight: torch.Size([2304, 384, 1, 1])
encoder.model.blocks.5.7.bn1.weight: torch.Size([2304])
encoder.model.blocks.5.7.bn1.bias: torch.Size([2304])
encoder.model.blocks.5.7.bn1.running_mean: torch.Size([2304])
encoder.model.blocks.5.7.bn1.running_var: torch.Size([2304])
encoder.model.blocks.5.7.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.7.conv_dw.weight: torch.Size([2304, 1, 3, 3])
encoder.model.blocks.5.7.bn2.weight: torch.Size([2304])
encoder.model.blocks.5.7.bn2.bias: torch.Size([2304])
encoder.model.blocks.5.7.bn2.running_mean: torch.Size([2304])
encoder.model.blocks.5.7.bn2.running_var: torch.Size([2304])
encoder.model.blocks.5.7.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.7.se.conv_reduce.weight: torch.Size([96, 2304, 1, 1])
encoder.model.blocks.5.7.se.conv_reduce.bias: torch.Size([96])
encoder.model.blocks.5.7.se.conv_expand.weight: torch.Size([2304, 96, 1, 1])
encoder.model.blocks.5.7.se.conv_expand.bias: torch.Size([2304])
encoder.model.blocks.5.7.conv_pwl.weight: torch.Size([384, 2304, 1, 1])
encoder.model.blocks.5.7.bn3.weight: torch.Size([384])
encoder.model.blocks.5.7.bn3.bias: torch.Size([384])
encoder.model.blocks.5.7.bn3.running_mean: torch.Size([384])
encoder.model.blocks.5.7.bn3.running_var: torch.Size([384])
encoder.model.blocks.5.7.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.8.conv_pw.weight: torch.Size([2304, 384, 1, 1])
encoder.model.blocks.5.8.bn1.weight: torch.Size([2304])
encoder.model.blocks.5.8.bn1.bias: torch.Size([2304])
encoder.model.blocks.5.8.bn1.running_mean: torch.Size([2304])
encoder.model.blocks.5.8.bn1.running_var: torch.Size([2304])
encoder.model.blocks.5.8.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.8.conv_dw.weight: torch.Size([2304, 1, 3, 3])
encoder.model.blocks.5.8.bn2.weight: torch.Size([2304])
encoder.model.blocks.5.8.bn2.bias: torch.Size([2304])
encoder.model.blocks.5.8.bn2.running_mean: torch.Size([2304])
encoder.model.blocks.5.8.bn2.running_var: torch.Size([2304])
encoder.model.blocks.5.8.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.8.se.conv_reduce.weight: torch.Size([96, 2304, 1, 1])
encoder.model.blocks.5.8.se.conv_reduce.bias: torch.Size([96])
encoder.model.blocks.5.8.se.conv_expand.weight: torch.Size([2304, 96, 1, 1])
encoder.model.blocks.5.8.se.conv_expand.bias: torch.Size([2304])
encoder.model.blocks.5.8.conv_pwl.weight: torch.Size([384, 2304, 1, 1])
encoder.model.blocks.5.8.bn3.weight: torch.Size([384])
encoder.model.blocks.5.8.bn3.bias: torch.Size([384])
encoder.model.blocks.5.8.bn3.running_mean: torch.Size([384])
encoder.model.blocks.5.8.bn3.running_var: torch.Size([384])
encoder.model.blocks.5.8.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.9.conv_pw.weight: torch.Size([2304, 384, 1, 1])
encoder.model.blocks.5.9.bn1.weight: torch.Size([2304])
encoder.model.blocks.5.9.bn1.bias: torch.Size([2304])
encoder.model.blocks.5.9.bn1.running_mean: torch.Size([2304])
encoder.model.blocks.5.9.bn1.running_var: torch.Size([2304])
encoder.model.blocks.5.9.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.9.conv_dw.weight: torch.Size([2304, 1, 3, 3])
encoder.model.blocks.5.9.bn2.weight: torch.Size([2304])
encoder.model.blocks.5.9.bn2.bias: torch.Size([2304])
encoder.model.blocks.5.9.bn2.running_mean: torch.Size([2304])
encoder.model.blocks.5.9.bn2.running_var: torch.Size([2304])
encoder.model.blocks.5.9.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.9.se.conv_reduce.weight: torch.Size([96, 2304, 1, 1])
encoder.model.blocks.5.9.se.conv_reduce.bias: torch.Size([96])
encoder.model.blocks.5.9.se.conv_expand.weight: torch.Size([2304, 96, 1, 1])
encoder.model.blocks.5.9.se.conv_expand.bias: torch.Size([2304])
encoder.model.blocks.5.9.conv_pwl.weight: torch.Size([384, 2304, 1, 1])
encoder.model.blocks.5.9.bn3.weight: torch.Size([384])
encoder.model.blocks.5.9.bn3.bias: torch.Size([384])
encoder.model.blocks.5.9.bn3.running_mean: torch.Size([384])
encoder.model.blocks.5.9.bn3.running_var: torch.Size([384])
encoder.model.blocks.5.9.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.10.conv_pw.weight: torch.Size([2304, 384, 1, 1])
encoder.model.blocks.5.10.bn1.weight: torch.Size([2304])
encoder.model.blocks.5.10.bn1.bias: torch.Size([2304])
encoder.model.blocks.5.10.bn1.running_mean: torch.Size([2304])
encoder.model.blocks.5.10.bn1.running_var: torch.Size([2304])
encoder.model.blocks.5.10.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.10.conv_dw.weight: torch.Size([2304, 1, 3, 3])
encoder.model.blocks.5.10.bn2.weight: torch.Size([2304])
encoder.model.blocks.5.10.bn2.bias: torch.Size([2304])
encoder.model.blocks.5.10.bn2.running_mean: torch.Size([2304])
encoder.model.blocks.5.10.bn2.running_var: torch.Size([2304])
encoder.model.blocks.5.10.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.10.se.conv_reduce.weight: torch.Size([96, 2304, 1, 1])
encoder.model.blocks.5.10.se.conv_reduce.bias: torch.Size([96])
encoder.model.blocks.5.10.se.conv_expand.weight: torch.Size([2304, 96, 1, 1])
encoder.model.blocks.5.10.se.conv_expand.bias: torch.Size([2304])
encoder.model.blocks.5.10.conv_pwl.weight: torch.Size([384, 2304, 1, 1])
encoder.model.blocks.5.10.bn3.weight: torch.Size([384])
encoder.model.blocks.5.10.bn3.bias: torch.Size([384])
encoder.model.blocks.5.10.bn3.running_mean: torch.Size([384])
encoder.model.blocks.5.10.bn3.running_var: torch.Size([384])
encoder.model.blocks.5.10.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.11.conv_pw.weight: torch.Size([2304, 384, 1, 1])
encoder.model.blocks.5.11.bn1.weight: torch.Size([2304])
encoder.model.blocks.5.11.bn1.bias: torch.Size([2304])
encoder.model.blocks.5.11.bn1.running_mean: torch.Size([2304])
encoder.model.blocks.5.11.bn1.running_var: torch.Size([2304])
encoder.model.blocks.5.11.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.11.conv_dw.weight: torch.Size([2304, 1, 3, 3])
encoder.model.blocks.5.11.bn2.weight: torch.Size([2304])
encoder.model.blocks.5.11.bn2.bias: torch.Size([2304])
encoder.model.blocks.5.11.bn2.running_mean: torch.Size([2304])
encoder.model.blocks.5.11.bn2.running_var: torch.Size([2304])
encoder.model.blocks.5.11.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.11.se.conv_reduce.weight: torch.Size([96, 2304, 1, 1])
encoder.model.blocks.5.11.se.conv_reduce.bias: torch.Size([96])
encoder.model.blocks.5.11.se.conv_expand.weight: torch.Size([2304, 96, 1, 1])
encoder.model.blocks.5.11.se.conv_expand.bias: torch.Size([2304])
encoder.model.blocks.5.11.conv_pwl.weight: torch.Size([384, 2304, 1, 1])
encoder.model.blocks.5.11.bn3.weight: torch.Size([384])
encoder.model.blocks.5.11.bn3.bias: torch.Size([384])
encoder.model.blocks.5.11.bn3.running_mean: torch.Size([384])
encoder.model.blocks.5.11.bn3.running_var: torch.Size([384])
encoder.model.blocks.5.11.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.12.conv_pw.weight: torch.Size([2304, 384, 1, 1])
encoder.model.blocks.5.12.bn1.weight: torch.Size([2304])
encoder.model.blocks.5.12.bn1.bias: torch.Size([2304])
encoder.model.blocks.5.12.bn1.running_mean: torch.Size([2304])
encoder.model.blocks.5.12.bn1.running_var: torch.Size([2304])
encoder.model.blocks.5.12.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.12.conv_dw.weight: torch.Size([2304, 1, 3, 3])
encoder.model.blocks.5.12.bn2.weight: torch.Size([2304])
encoder.model.blocks.5.12.bn2.bias: torch.Size([2304])
encoder.model.blocks.5.12.bn2.running_mean: torch.Size([2304])
encoder.model.blocks.5.12.bn2.running_var: torch.Size([2304])
encoder.model.blocks.5.12.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.12.se.conv_reduce.weight: torch.Size([96, 2304, 1, 1])
encoder.model.blocks.5.12.se.conv_reduce.bias: torch.Size([96])
encoder.model.blocks.5.12.se.conv_expand.weight: torch.Size([2304, 96, 1, 1])
encoder.model.blocks.5.12.se.conv_expand.bias: torch.Size([2304])
encoder.model.blocks.5.12.conv_pwl.weight: torch.Size([384, 2304, 1, 1])
encoder.model.blocks.5.12.bn3.weight: torch.Size([384])
encoder.model.blocks.5.12.bn3.bias: torch.Size([384])
encoder.model.blocks.5.12.bn3.running_mean: torch.Size([384])
encoder.model.blocks.5.12.bn3.running_var: torch.Size([384])
encoder.model.blocks.5.12.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.13.conv_pw.weight: torch.Size([2304, 384, 1, 1])
encoder.model.blocks.5.13.bn1.weight: torch.Size([2304])
encoder.model.blocks.5.13.bn1.bias: torch.Size([2304])
encoder.model.blocks.5.13.bn1.running_mean: torch.Size([2304])
encoder.model.blocks.5.13.bn1.running_var: torch.Size([2304])
encoder.model.blocks.5.13.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.13.conv_dw.weight: torch.Size([2304, 1, 3, 3])
encoder.model.blocks.5.13.bn2.weight: torch.Size([2304])
encoder.model.blocks.5.13.bn2.bias: torch.Size([2304])
encoder.model.blocks.5.13.bn2.running_mean: torch.Size([2304])
encoder.model.blocks.5.13.bn2.running_var: torch.Size([2304])
encoder.model.blocks.5.13.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.13.se.conv_reduce.weight: torch.Size([96, 2304, 1, 1])
encoder.model.blocks.5.13.se.conv_reduce.bias: torch.Size([96])
encoder.model.blocks.5.13.se.conv_expand.weight: torch.Size([2304, 96, 1, 1])
encoder.model.blocks.5.13.se.conv_expand.bias: torch.Size([2304])
encoder.model.blocks.5.13.conv_pwl.weight: torch.Size([384, 2304, 1, 1])
encoder.model.blocks.5.13.bn3.weight: torch.Size([384])
encoder.model.blocks.5.13.bn3.bias: torch.Size([384])
encoder.model.blocks.5.13.bn3.running_mean: torch.Size([384])
encoder.model.blocks.5.13.bn3.running_var: torch.Size([384])
encoder.model.blocks.5.13.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.14.conv_pw.weight: torch.Size([2304, 384, 1, 1])
encoder.model.blocks.5.14.bn1.weight: torch.Size([2304])
encoder.model.blocks.5.14.bn1.bias: torch.Size([2304])
encoder.model.blocks.5.14.bn1.running_mean: torch.Size([2304])
encoder.model.blocks.5.14.bn1.running_var: torch.Size([2304])
encoder.model.blocks.5.14.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.14.conv_dw.weight: torch.Size([2304, 1, 3, 3])
encoder.model.blocks.5.14.bn2.weight: torch.Size([2304])
encoder.model.blocks.5.14.bn2.bias: torch.Size([2304])
encoder.model.blocks.5.14.bn2.running_mean: torch.Size([2304])
encoder.model.blocks.5.14.bn2.running_var: torch.Size([2304])
encoder.model.blocks.5.14.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.14.se.conv_reduce.weight: torch.Size([96, 2304, 1, 1])
encoder.model.blocks.5.14.se.conv_reduce.bias: torch.Size([96])
encoder.model.blocks.5.14.se.conv_expand.weight: torch.Size([2304, 96, 1, 1])
encoder.model.blocks.5.14.se.conv_expand.bias: torch.Size([2304])
encoder.model.blocks.5.14.conv_pwl.weight: torch.Size([384, 2304, 1, 1])
encoder.model.blocks.5.14.bn3.weight: torch.Size([384])
encoder.model.blocks.5.14.bn3.bias: torch.Size([384])
encoder.model.blocks.5.14.bn3.running_mean: torch.Size([384])
encoder.model.blocks.5.14.bn3.running_var: torch.Size([384])
encoder.model.blocks.5.14.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.15.conv_pw.weight: torch.Size([2304, 384, 1, 1])
encoder.model.blocks.5.15.bn1.weight: torch.Size([2304])
encoder.model.blocks.5.15.bn1.bias: torch.Size([2304])
encoder.model.blocks.5.15.bn1.running_mean: torch.Size([2304])
encoder.model.blocks.5.15.bn1.running_var: torch.Size([2304])
encoder.model.blocks.5.15.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.15.conv_dw.weight: torch.Size([2304, 1, 3, 3])
encoder.model.blocks.5.15.bn2.weight: torch.Size([2304])
encoder.model.blocks.5.15.bn2.bias: torch.Size([2304])
encoder.model.blocks.5.15.bn2.running_mean: torch.Size([2304])
encoder.model.blocks.5.15.bn2.running_var: torch.Size([2304])
encoder.model.blocks.5.15.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.15.se.conv_reduce.weight: torch.Size([96, 2304, 1, 1])
encoder.model.blocks.5.15.se.conv_reduce.bias: torch.Size([96])
encoder.model.blocks.5.15.se.conv_expand.weight: torch.Size([2304, 96, 1, 1])
encoder.model.blocks.5.15.se.conv_expand.bias: torch.Size([2304])
encoder.model.blocks.5.15.conv_pwl.weight: torch.Size([384, 2304, 1, 1])
encoder.model.blocks.5.15.bn3.weight: torch.Size([384])
encoder.model.blocks.5.15.bn3.bias: torch.Size([384])
encoder.model.blocks.5.15.bn3.running_mean: torch.Size([384])
encoder.model.blocks.5.15.bn3.running_var: torch.Size([384])
encoder.model.blocks.5.15.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.16.conv_pw.weight: torch.Size([2304, 384, 1, 1])
encoder.model.blocks.5.16.bn1.weight: torch.Size([2304])
encoder.model.blocks.5.16.bn1.bias: torch.Size([2304])
encoder.model.blocks.5.16.bn1.running_mean: torch.Size([2304])
encoder.model.blocks.5.16.bn1.running_var: torch.Size([2304])
encoder.model.blocks.5.16.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.16.conv_dw.weight: torch.Size([2304, 1, 3, 3])
encoder.model.blocks.5.16.bn2.weight: torch.Size([2304])
encoder.model.blocks.5.16.bn2.bias: torch.Size([2304])
encoder.model.blocks.5.16.bn2.running_mean: torch.Size([2304])
encoder.model.blocks.5.16.bn2.running_var: torch.Size([2304])
encoder.model.blocks.5.16.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.16.se.conv_reduce.weight: torch.Size([96, 2304, 1, 1])
encoder.model.blocks.5.16.se.conv_reduce.bias: torch.Size([96])
encoder.model.blocks.5.16.se.conv_expand.weight: torch.Size([2304, 96, 1, 1])
encoder.model.blocks.5.16.se.conv_expand.bias: torch.Size([2304])
encoder.model.blocks.5.16.conv_pwl.weight: torch.Size([384, 2304, 1, 1])
encoder.model.blocks.5.16.bn3.weight: torch.Size([384])
encoder.model.blocks.5.16.bn3.bias: torch.Size([384])
encoder.model.blocks.5.16.bn3.running_mean: torch.Size([384])
encoder.model.blocks.5.16.bn3.running_var: torch.Size([384])
encoder.model.blocks.5.16.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.17.conv_pw.weight: torch.Size([2304, 384, 1, 1])
encoder.model.blocks.5.17.bn1.weight: torch.Size([2304])
encoder.model.blocks.5.17.bn1.bias: torch.Size([2304])
encoder.model.blocks.5.17.bn1.running_mean: torch.Size([2304])
encoder.model.blocks.5.17.bn1.running_var: torch.Size([2304])
encoder.model.blocks.5.17.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.17.conv_dw.weight: torch.Size([2304, 1, 3, 3])
encoder.model.blocks.5.17.bn2.weight: torch.Size([2304])
encoder.model.blocks.5.17.bn2.bias: torch.Size([2304])
encoder.model.blocks.5.17.bn2.running_mean: torch.Size([2304])
encoder.model.blocks.5.17.bn2.running_var: torch.Size([2304])
encoder.model.blocks.5.17.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.17.se.conv_reduce.weight: torch.Size([96, 2304, 1, 1])
encoder.model.blocks.5.17.se.conv_reduce.bias: torch.Size([96])
encoder.model.blocks.5.17.se.conv_expand.weight: torch.Size([2304, 96, 1, 1])
encoder.model.blocks.5.17.se.conv_expand.bias: torch.Size([2304])
encoder.model.blocks.5.17.conv_pwl.weight: torch.Size([384, 2304, 1, 1])
encoder.model.blocks.5.17.bn3.weight: torch.Size([384])
encoder.model.blocks.5.17.bn3.bias: torch.Size([384])
encoder.model.blocks.5.17.bn3.running_mean: torch.Size([384])
encoder.model.blocks.5.17.bn3.running_var: torch.Size([384])
encoder.model.blocks.5.17.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.18.conv_pw.weight: torch.Size([2304, 384, 1, 1])
encoder.model.blocks.5.18.bn1.weight: torch.Size([2304])
encoder.model.blocks.5.18.bn1.bias: torch.Size([2304])
encoder.model.blocks.5.18.bn1.running_mean: torch.Size([2304])
encoder.model.blocks.5.18.bn1.running_var: torch.Size([2304])
encoder.model.blocks.5.18.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.18.conv_dw.weight: torch.Size([2304, 1, 3, 3])
encoder.model.blocks.5.18.bn2.weight: torch.Size([2304])
encoder.model.blocks.5.18.bn2.bias: torch.Size([2304])
encoder.model.blocks.5.18.bn2.running_mean: torch.Size([2304])
encoder.model.blocks.5.18.bn2.running_var: torch.Size([2304])
encoder.model.blocks.5.18.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.18.se.conv_reduce.weight: torch.Size([96, 2304, 1, 1])
encoder.model.blocks.5.18.se.conv_reduce.bias: torch.Size([96])
encoder.model.blocks.5.18.se.conv_expand.weight: torch.Size([2304, 96, 1, 1])
encoder.model.blocks.5.18.se.conv_expand.bias: torch.Size([2304])
encoder.model.blocks.5.18.conv_pwl.weight: torch.Size([384, 2304, 1, 1])
encoder.model.blocks.5.18.bn3.weight: torch.Size([384])
encoder.model.blocks.5.18.bn3.bias: torch.Size([384])
encoder.model.blocks.5.18.bn3.running_mean: torch.Size([384])
encoder.model.blocks.5.18.bn3.running_var: torch.Size([384])
encoder.model.blocks.5.18.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.19.conv_pw.weight: torch.Size([2304, 384, 1, 1])
encoder.model.blocks.5.19.bn1.weight: torch.Size([2304])
encoder.model.blocks.5.19.bn1.bias: torch.Size([2304])
encoder.model.blocks.5.19.bn1.running_mean: torch.Size([2304])
encoder.model.blocks.5.19.bn1.running_var: torch.Size([2304])
encoder.model.blocks.5.19.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.19.conv_dw.weight: torch.Size([2304, 1, 3, 3])
encoder.model.blocks.5.19.bn2.weight: torch.Size([2304])
encoder.model.blocks.5.19.bn2.bias: torch.Size([2304])
encoder.model.blocks.5.19.bn2.running_mean: torch.Size([2304])
encoder.model.blocks.5.19.bn2.running_var: torch.Size([2304])
encoder.model.blocks.5.19.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.19.se.conv_reduce.weight: torch.Size([96, 2304, 1, 1])
encoder.model.blocks.5.19.se.conv_reduce.bias: torch.Size([96])
encoder.model.blocks.5.19.se.conv_expand.weight: torch.Size([2304, 96, 1, 1])
encoder.model.blocks.5.19.se.conv_expand.bias: torch.Size([2304])
encoder.model.blocks.5.19.conv_pwl.weight: torch.Size([384, 2304, 1, 1])
encoder.model.blocks.5.19.bn3.weight: torch.Size([384])
encoder.model.blocks.5.19.bn3.bias: torch.Size([384])
encoder.model.blocks.5.19.bn3.running_mean: torch.Size([384])
encoder.model.blocks.5.19.bn3.running_var: torch.Size([384])
encoder.model.blocks.5.19.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.20.conv_pw.weight: torch.Size([2304, 384, 1, 1])
encoder.model.blocks.5.20.bn1.weight: torch.Size([2304])
encoder.model.blocks.5.20.bn1.bias: torch.Size([2304])
encoder.model.blocks.5.20.bn1.running_mean: torch.Size([2304])
encoder.model.blocks.5.20.bn1.running_var: torch.Size([2304])
encoder.model.blocks.5.20.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.20.conv_dw.weight: torch.Size([2304, 1, 3, 3])
encoder.model.blocks.5.20.bn2.weight: torch.Size([2304])
encoder.model.blocks.5.20.bn2.bias: torch.Size([2304])
encoder.model.blocks.5.20.bn2.running_mean: torch.Size([2304])
encoder.model.blocks.5.20.bn2.running_var: torch.Size([2304])
encoder.model.blocks.5.20.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.20.se.conv_reduce.weight: torch.Size([96, 2304, 1, 1])
encoder.model.blocks.5.20.se.conv_reduce.bias: torch.Size([96])
encoder.model.blocks.5.20.se.conv_expand.weight: torch.Size([2304, 96, 1, 1])
encoder.model.blocks.5.20.se.conv_expand.bias: torch.Size([2304])
encoder.model.blocks.5.20.conv_pwl.weight: torch.Size([384, 2304, 1, 1])
encoder.model.blocks.5.20.bn3.weight: torch.Size([384])
encoder.model.blocks.5.20.bn3.bias: torch.Size([384])
encoder.model.blocks.5.20.bn3.running_mean: torch.Size([384])
encoder.model.blocks.5.20.bn3.running_var: torch.Size([384])
encoder.model.blocks.5.20.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.21.conv_pw.weight: torch.Size([2304, 384, 1, 1])
encoder.model.blocks.5.21.bn1.weight: torch.Size([2304])
encoder.model.blocks.5.21.bn1.bias: torch.Size([2304])
encoder.model.blocks.5.21.bn1.running_mean: torch.Size([2304])
encoder.model.blocks.5.21.bn1.running_var: torch.Size([2304])
encoder.model.blocks.5.21.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.21.conv_dw.weight: torch.Size([2304, 1, 3, 3])
encoder.model.blocks.5.21.bn2.weight: torch.Size([2304])
encoder.model.blocks.5.21.bn2.bias: torch.Size([2304])
encoder.model.blocks.5.21.bn2.running_mean: torch.Size([2304])
encoder.model.blocks.5.21.bn2.running_var: torch.Size([2304])
encoder.model.blocks.5.21.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.21.se.conv_reduce.weight: torch.Size([96, 2304, 1, 1])
encoder.model.blocks.5.21.se.conv_reduce.bias: torch.Size([96])
encoder.model.blocks.5.21.se.conv_expand.weight: torch.Size([2304, 96, 1, 1])
encoder.model.blocks.5.21.se.conv_expand.bias: torch.Size([2304])
encoder.model.blocks.5.21.conv_pwl.weight: torch.Size([384, 2304, 1, 1])
encoder.model.blocks.5.21.bn3.weight: torch.Size([384])
encoder.model.blocks.5.21.bn3.bias: torch.Size([384])
encoder.model.blocks.5.21.bn3.running_mean: torch.Size([384])
encoder.model.blocks.5.21.bn3.running_var: torch.Size([384])
encoder.model.blocks.5.21.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.22.conv_pw.weight: torch.Size([2304, 384, 1, 1])
encoder.model.blocks.5.22.bn1.weight: torch.Size([2304])
encoder.model.blocks.5.22.bn1.bias: torch.Size([2304])
encoder.model.blocks.5.22.bn1.running_mean: torch.Size([2304])
encoder.model.blocks.5.22.bn1.running_var: torch.Size([2304])
encoder.model.blocks.5.22.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.22.conv_dw.weight: torch.Size([2304, 1, 3, 3])
encoder.model.blocks.5.22.bn2.weight: torch.Size([2304])
encoder.model.blocks.5.22.bn2.bias: torch.Size([2304])
encoder.model.blocks.5.22.bn2.running_mean: torch.Size([2304])
encoder.model.blocks.5.22.bn2.running_var: torch.Size([2304])
encoder.model.blocks.5.22.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.22.se.conv_reduce.weight: torch.Size([96, 2304, 1, 1])
encoder.model.blocks.5.22.se.conv_reduce.bias: torch.Size([96])
encoder.model.blocks.5.22.se.conv_expand.weight: torch.Size([2304, 96, 1, 1])
encoder.model.blocks.5.22.se.conv_expand.bias: torch.Size([2304])
encoder.model.blocks.5.22.conv_pwl.weight: torch.Size([384, 2304, 1, 1])
encoder.model.blocks.5.22.bn3.weight: torch.Size([384])
encoder.model.blocks.5.22.bn3.bias: torch.Size([384])
encoder.model.blocks.5.22.bn3.running_mean: torch.Size([384])
encoder.model.blocks.5.22.bn3.running_var: torch.Size([384])
encoder.model.blocks.5.22.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.23.conv_pw.weight: torch.Size([2304, 384, 1, 1])
encoder.model.blocks.5.23.bn1.weight: torch.Size([2304])
encoder.model.blocks.5.23.bn1.bias: torch.Size([2304])
encoder.model.blocks.5.23.bn1.running_mean: torch.Size([2304])
encoder.model.blocks.5.23.bn1.running_var: torch.Size([2304])
encoder.model.blocks.5.23.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.23.conv_dw.weight: torch.Size([2304, 1, 3, 3])
encoder.model.blocks.5.23.bn2.weight: torch.Size([2304])
encoder.model.blocks.5.23.bn2.bias: torch.Size([2304])
encoder.model.blocks.5.23.bn2.running_mean: torch.Size([2304])
encoder.model.blocks.5.23.bn2.running_var: torch.Size([2304])
encoder.model.blocks.5.23.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.23.se.conv_reduce.weight: torch.Size([96, 2304, 1, 1])
encoder.model.blocks.5.23.se.conv_reduce.bias: torch.Size([96])
encoder.model.blocks.5.23.se.conv_expand.weight: torch.Size([2304, 96, 1, 1])
encoder.model.blocks.5.23.se.conv_expand.bias: torch.Size([2304])
encoder.model.blocks.5.23.conv_pwl.weight: torch.Size([384, 2304, 1, 1])
encoder.model.blocks.5.23.bn3.weight: torch.Size([384])
encoder.model.blocks.5.23.bn3.bias: torch.Size([384])
encoder.model.blocks.5.23.bn3.running_mean: torch.Size([384])
encoder.model.blocks.5.23.bn3.running_var: torch.Size([384])
encoder.model.blocks.5.23.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.24.conv_pw.weight: torch.Size([2304, 384, 1, 1])
encoder.model.blocks.5.24.bn1.weight: torch.Size([2304])
encoder.model.blocks.5.24.bn1.bias: torch.Size([2304])
encoder.model.blocks.5.24.bn1.running_mean: torch.Size([2304])
encoder.model.blocks.5.24.bn1.running_var: torch.Size([2304])
encoder.model.blocks.5.24.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.24.conv_dw.weight: torch.Size([2304, 1, 3, 3])
encoder.model.blocks.5.24.bn2.weight: torch.Size([2304])
encoder.model.blocks.5.24.bn2.bias: torch.Size([2304])
encoder.model.blocks.5.24.bn2.running_mean: torch.Size([2304])
encoder.model.blocks.5.24.bn2.running_var: torch.Size([2304])
encoder.model.blocks.5.24.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.5.24.se.conv_reduce.weight: torch.Size([96, 2304, 1, 1])
encoder.model.blocks.5.24.se.conv_reduce.bias: torch.Size([96])
encoder.model.blocks.5.24.se.conv_expand.weight: torch.Size([2304, 96, 1, 1])
encoder.model.blocks.5.24.se.conv_expand.bias: torch.Size([2304])
encoder.model.blocks.5.24.conv_pwl.weight: torch.Size([384, 2304, 1, 1])
encoder.model.blocks.5.24.bn3.weight: torch.Size([384])
encoder.model.blocks.5.24.bn3.bias: torch.Size([384])
encoder.model.blocks.5.24.bn3.running_mean: torch.Size([384])
encoder.model.blocks.5.24.bn3.running_var: torch.Size([384])
encoder.model.blocks.5.24.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.6.0.conv_pw.weight: torch.Size([2304, 384, 1, 1])
encoder.model.blocks.6.0.bn1.weight: torch.Size([2304])
encoder.model.blocks.6.0.bn1.bias: torch.Size([2304])
encoder.model.blocks.6.0.bn1.running_mean: torch.Size([2304])
encoder.model.blocks.6.0.bn1.running_var: torch.Size([2304])
encoder.model.blocks.6.0.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.6.0.conv_dw.weight: torch.Size([2304, 1, 3, 3])
encoder.model.blocks.6.0.bn2.weight: torch.Size([2304])
encoder.model.blocks.6.0.bn2.bias: torch.Size([2304])
encoder.model.blocks.6.0.bn2.running_mean: torch.Size([2304])
encoder.model.blocks.6.0.bn2.running_var: torch.Size([2304])
encoder.model.blocks.6.0.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.6.0.se.conv_reduce.weight: torch.Size([96, 2304, 1, 1])
encoder.model.blocks.6.0.se.conv_reduce.bias: torch.Size([96])
encoder.model.blocks.6.0.se.conv_expand.weight: torch.Size([2304, 96, 1, 1])
encoder.model.blocks.6.0.se.conv_expand.bias: torch.Size([2304])
encoder.model.blocks.6.0.conv_pwl.weight: torch.Size([640, 2304, 1, 1])
encoder.model.blocks.6.0.bn3.weight: torch.Size([640])
encoder.model.blocks.6.0.bn3.bias: torch.Size([640])
encoder.model.blocks.6.0.bn3.running_mean: torch.Size([640])
encoder.model.blocks.6.0.bn3.running_var: torch.Size([640])
encoder.model.blocks.6.0.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.6.1.conv_pw.weight: torch.Size([3840, 640, 1, 1])
encoder.model.blocks.6.1.bn1.weight: torch.Size([3840])
encoder.model.blocks.6.1.bn1.bias: torch.Size([3840])
encoder.model.blocks.6.1.bn1.running_mean: torch.Size([3840])
encoder.model.blocks.6.1.bn1.running_var: torch.Size([3840])
encoder.model.blocks.6.1.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.6.1.conv_dw.weight: torch.Size([3840, 1, 3, 3])
encoder.model.blocks.6.1.bn2.weight: torch.Size([3840])
encoder.model.blocks.6.1.bn2.bias: torch.Size([3840])
encoder.model.blocks.6.1.bn2.running_mean: torch.Size([3840])
encoder.model.blocks.6.1.bn2.running_var: torch.Size([3840])
encoder.model.blocks.6.1.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.6.1.se.conv_reduce.weight: torch.Size([160, 3840, 1, 1])
encoder.model.blocks.6.1.se.conv_reduce.bias: torch.Size([160])
encoder.model.blocks.6.1.se.conv_expand.weight: torch.Size([3840, 160, 1, 1])
encoder.model.blocks.6.1.se.conv_expand.bias: torch.Size([3840])
encoder.model.blocks.6.1.conv_pwl.weight: torch.Size([640, 3840, 1, 1])
encoder.model.blocks.6.1.bn3.weight: torch.Size([640])
encoder.model.blocks.6.1.bn3.bias: torch.Size([640])
encoder.model.blocks.6.1.bn3.running_mean: torch.Size([640])
encoder.model.blocks.6.1.bn3.running_var: torch.Size([640])
encoder.model.blocks.6.1.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.6.2.conv_pw.weight: torch.Size([3840, 640, 1, 1])
encoder.model.blocks.6.2.bn1.weight: torch.Size([3840])
encoder.model.blocks.6.2.bn1.bias: torch.Size([3840])
encoder.model.blocks.6.2.bn1.running_mean: torch.Size([3840])
encoder.model.blocks.6.2.bn1.running_var: torch.Size([3840])
encoder.model.blocks.6.2.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.6.2.conv_dw.weight: torch.Size([3840, 1, 3, 3])
encoder.model.blocks.6.2.bn2.weight: torch.Size([3840])
encoder.model.blocks.6.2.bn2.bias: torch.Size([3840])
encoder.model.blocks.6.2.bn2.running_mean: torch.Size([3840])
encoder.model.blocks.6.2.bn2.running_var: torch.Size([3840])
encoder.model.blocks.6.2.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.6.2.se.conv_reduce.weight: torch.Size([160, 3840, 1, 1])
encoder.model.blocks.6.2.se.conv_reduce.bias: torch.Size([160])
encoder.model.blocks.6.2.se.conv_expand.weight: torch.Size([3840, 160, 1, 1])
encoder.model.blocks.6.2.se.conv_expand.bias: torch.Size([3840])
encoder.model.blocks.6.2.conv_pwl.weight: torch.Size([640, 3840, 1, 1])
encoder.model.blocks.6.2.bn3.weight: torch.Size([640])
encoder.model.blocks.6.2.bn3.bias: torch.Size([640])
encoder.model.blocks.6.2.bn3.running_mean: torch.Size([640])
encoder.model.blocks.6.2.bn3.running_var: torch.Size([640])
encoder.model.blocks.6.2.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.6.3.conv_pw.weight: torch.Size([3840, 640, 1, 1])
encoder.model.blocks.6.3.bn1.weight: torch.Size([3840])
encoder.model.blocks.6.3.bn1.bias: torch.Size([3840])
encoder.model.blocks.6.3.bn1.running_mean: torch.Size([3840])
encoder.model.blocks.6.3.bn1.running_var: torch.Size([3840])
encoder.model.blocks.6.3.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.6.3.conv_dw.weight: torch.Size([3840, 1, 3, 3])
encoder.model.blocks.6.3.bn2.weight: torch.Size([3840])
encoder.model.blocks.6.3.bn2.bias: torch.Size([3840])
encoder.model.blocks.6.3.bn2.running_mean: torch.Size([3840])
encoder.model.blocks.6.3.bn2.running_var: torch.Size([3840])
encoder.model.blocks.6.3.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.6.3.se.conv_reduce.weight: torch.Size([160, 3840, 1, 1])
encoder.model.blocks.6.3.se.conv_reduce.bias: torch.Size([160])
encoder.model.blocks.6.3.se.conv_expand.weight: torch.Size([3840, 160, 1, 1])
encoder.model.blocks.6.3.se.conv_expand.bias: torch.Size([3840])
encoder.model.blocks.6.3.conv_pwl.weight: torch.Size([640, 3840, 1, 1])
encoder.model.blocks.6.3.bn3.weight: torch.Size([640])
encoder.model.blocks.6.3.bn3.bias: torch.Size([640])
encoder.model.blocks.6.3.bn3.running_mean: torch.Size([640])
encoder.model.blocks.6.3.bn3.running_var: torch.Size([640])
encoder.model.blocks.6.3.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.6.4.conv_pw.weight: torch.Size([3840, 640, 1, 1])
encoder.model.blocks.6.4.bn1.weight: torch.Size([3840])
encoder.model.blocks.6.4.bn1.bias: torch.Size([3840])
encoder.model.blocks.6.4.bn1.running_mean: torch.Size([3840])
encoder.model.blocks.6.4.bn1.running_var: torch.Size([3840])
encoder.model.blocks.6.4.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.6.4.conv_dw.weight: torch.Size([3840, 1, 3, 3])
encoder.model.blocks.6.4.bn2.weight: torch.Size([3840])
encoder.model.blocks.6.4.bn2.bias: torch.Size([3840])
encoder.model.blocks.6.4.bn2.running_mean: torch.Size([3840])
encoder.model.blocks.6.4.bn2.running_var: torch.Size([3840])
encoder.model.blocks.6.4.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.6.4.se.conv_reduce.weight: torch.Size([160, 3840, 1, 1])
encoder.model.blocks.6.4.se.conv_reduce.bias: torch.Size([160])
encoder.model.blocks.6.4.se.conv_expand.weight: torch.Size([3840, 160, 1, 1])
encoder.model.blocks.6.4.se.conv_expand.bias: torch.Size([3840])
encoder.model.blocks.6.4.conv_pwl.weight: torch.Size([640, 3840, 1, 1])
encoder.model.blocks.6.4.bn3.weight: torch.Size([640])
encoder.model.blocks.6.4.bn3.bias: torch.Size([640])
encoder.model.blocks.6.4.bn3.running_mean: torch.Size([640])
encoder.model.blocks.6.4.bn3.running_var: torch.Size([640])
encoder.model.blocks.6.4.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.6.5.conv_pw.weight: torch.Size([3840, 640, 1, 1])
encoder.model.blocks.6.5.bn1.weight: torch.Size([3840])
encoder.model.blocks.6.5.bn1.bias: torch.Size([3840])
encoder.model.blocks.6.5.bn1.running_mean: torch.Size([3840])
encoder.model.blocks.6.5.bn1.running_var: torch.Size([3840])
encoder.model.blocks.6.5.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.6.5.conv_dw.weight: torch.Size([3840, 1, 3, 3])
encoder.model.blocks.6.5.bn2.weight: torch.Size([3840])
encoder.model.blocks.6.5.bn2.bias: torch.Size([3840])
encoder.model.blocks.6.5.bn2.running_mean: torch.Size([3840])
encoder.model.blocks.6.5.bn2.running_var: torch.Size([3840])
encoder.model.blocks.6.5.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.6.5.se.conv_reduce.weight: torch.Size([160, 3840, 1, 1])
encoder.model.blocks.6.5.se.conv_reduce.bias: torch.Size([160])
encoder.model.blocks.6.5.se.conv_expand.weight: torch.Size([3840, 160, 1, 1])
encoder.model.blocks.6.5.se.conv_expand.bias: torch.Size([3840])
encoder.model.blocks.6.5.conv_pwl.weight: torch.Size([640, 3840, 1, 1])
encoder.model.blocks.6.5.bn3.weight: torch.Size([640])
encoder.model.blocks.6.5.bn3.bias: torch.Size([640])
encoder.model.blocks.6.5.bn3.running_mean: torch.Size([640])
encoder.model.blocks.6.5.bn3.running_var: torch.Size([640])
encoder.model.blocks.6.5.bn3.num_batches_tracked: torch.Size([])
encoder.model.blocks.6.6.conv_pw.weight: torch.Size([3840, 640, 1, 1])
encoder.model.blocks.6.6.bn1.weight: torch.Size([3840])
encoder.model.blocks.6.6.bn1.bias: torch.Size([3840])
encoder.model.blocks.6.6.bn1.running_mean: torch.Size([3840])
encoder.model.blocks.6.6.bn1.running_var: torch.Size([3840])
encoder.model.blocks.6.6.bn1.num_batches_tracked: torch.Size([])
encoder.model.blocks.6.6.conv_dw.weight: torch.Size([3840, 1, 3, 3])
encoder.model.blocks.6.6.bn2.weight: torch.Size([3840])
encoder.model.blocks.6.6.bn2.bias: torch.Size([3840])
encoder.model.blocks.6.6.bn2.running_mean: torch.Size([3840])
encoder.model.blocks.6.6.bn2.running_var: torch.Size([3840])
encoder.model.blocks.6.6.bn2.num_batches_tracked: torch.Size([])
encoder.model.blocks.6.6.se.conv_reduce.weight: torch.Size([160, 3840, 1, 1])
encoder.model.blocks.6.6.se.conv_reduce.bias: torch.Size([160])
encoder.model.blocks.6.6.se.conv_expand.weight: torch.Size([3840, 160, 1, 1])
encoder.model.blocks.6.6.se.conv_expand.bias: torch.Size([3840])
encoder.model.blocks.6.6.conv_pwl.weight: torch.Size([640, 3840, 1, 1])
encoder.model.blocks.6.6.bn3.weight: torch.Size([640])
encoder.model.blocks.6.6.bn3.bias: torch.Size([640])
encoder.model.blocks.6.6.bn3.running_mean: torch.Size([640])
encoder.model.blocks.6.6.bn3.running_var: torch.Size([640])
encoder.model.blocks.6.6.bn3.num_batches_tracked: torch.Size([])
attn.0.attn.0.weight: torch.Size([32, 32])
attn.0.attn.0.bias: torch.Size([32])
attn.0.attn.1.weight: torch.Size([32])
attn.0.attn.1.bias: torch.Size([32])
attn.0.attn.3.weight: torch.Size([1, 32])
attn.0.attn.3.bias: torch.Size([1])
attn.1.attn.0.weight: torch.Size([64, 64])
attn.1.attn.0.bias: torch.Size([64])
attn.1.attn.1.weight: torch.Size([64])
attn.1.attn.1.bias: torch.Size([64])
attn.1.attn.3.weight: torch.Size([1, 64])
attn.1.attn.3.bias: torch.Size([1])
attn.2.attn.0.weight: torch.Size([96, 96])
attn.2.attn.0.bias: torch.Size([96])
attn.2.attn.1.weight: torch.Size([96])
attn.2.attn.1.bias: torch.Size([96])
attn.2.attn.3.weight: torch.Size([1, 96])
attn.2.attn.3.bias: torch.Size([1])
attn.3.attn.0.weight: torch.Size([224, 224])
attn.3.attn.0.bias: torch.Size([224])
attn.3.attn.1.weight: torch.Size([224])
attn.3.attn.1.bias: torch.Size([224])
attn.3.attn.3.weight: torch.Size([1, 224])
attn.3.attn.3.bias: torch.Size([1])
attn.4.attn.0.weight: torch.Size([640, 640])
attn.4.attn.0.bias: torch.Size([640])
attn.4.attn.1.weight: torch.Size([640])
attn.4.attn.1.bias: torch.Size([640])
attn.4.attn.3.weight: torch.Size([1, 640])
attn.4.attn.3.bias: torch.Size([1])
decoder.blocks.0.conv1.0.weight: torch.Size([384, 864, 3, 3])
decoder.blocks.0.conv1.1.weight: torch.Size([384])
decoder.blocks.0.conv1.1.bias: torch.Size([384])
decoder.blocks.0.conv1.1.running_mean: torch.Size([384])
decoder.blocks.0.conv1.1.running_var: torch.Size([384])
decoder.blocks.0.conv1.1.num_batches_tracked: torch.Size([])
decoder.blocks.0.attention1.attention.cSE.1.weight: torch.Size([54, 864, 1, 1])
decoder.blocks.0.attention1.attention.cSE.1.bias: torch.Size([54])
decoder.blocks.0.attention1.attention.cSE.3.weight: torch.Size([864, 54, 1, 1])
decoder.blocks.0.attention1.attention.cSE.3.bias: torch.Size([864])
decoder.blocks.0.attention1.attention.sSE.0.weight: torch.Size([1, 864, 1, 1])
decoder.blocks.0.attention1.attention.sSE.0.bias: torch.Size([1])
decoder.blocks.0.conv2.0.weight: torch.Size([384, 384, 3, 3])
decoder.blocks.0.conv2.1.weight: torch.Size([384])
decoder.blocks.0.conv2.1.bias: torch.Size([384])
decoder.blocks.0.conv2.1.running_mean: torch.Size([384])
decoder.blocks.0.conv2.1.running_var: torch.Size([384])
decoder.blocks.0.conv2.1.num_batches_tracked: torch.Size([])
decoder.blocks.0.attention2.attention.cSE.1.weight: torch.Size([24, 384, 1, 1])
decoder.blocks.0.attention2.attention.cSE.1.bias: torch.Size([24])
decoder.blocks.0.attention2.attention.cSE.3.weight: torch.Size([384, 24, 1, 1])
decoder.blocks.0.attention2.attention.cSE.3.bias: torch.Size([384])
decoder.blocks.0.attention2.attention.sSE.0.weight: torch.Size([1, 384, 1, 1])
decoder.blocks.0.attention2.attention.sSE.0.bias: torch.Size([1])
decoder.blocks.1.conv1.0.weight: torch.Size([368, 480, 3, 3])
decoder.blocks.1.conv1.1.weight: torch.Size([368])
decoder.blocks.1.conv1.1.bias: torch.Size([368])
decoder.blocks.1.conv1.1.running_mean: torch.Size([368])
decoder.blocks.1.conv1.1.running_var: torch.Size([368])
decoder.blocks.1.conv1.1.num_batches_tracked: torch.Size([])
decoder.blocks.1.attention1.attention.cSE.1.weight: torch.Size([30, 480, 1, 1])
decoder.blocks.1.attention1.attention.cSE.1.bias: torch.Size([30])
decoder.blocks.1.attention1.attention.cSE.3.weight: torch.Size([480, 30, 1, 1])
decoder.blocks.1.attention1.attention.cSE.3.bias: torch.Size([480])
decoder.blocks.1.attention1.attention.sSE.0.weight: torch.Size([1, 480, 1, 1])
decoder.blocks.1.attention1.attention.sSE.0.bias: torch.Size([1])
decoder.blocks.1.conv2.0.weight: torch.Size([368, 368, 3, 3])
decoder.blocks.1.conv2.1.weight: torch.Size([368])
decoder.blocks.1.conv2.1.bias: torch.Size([368])
decoder.blocks.1.conv2.1.running_mean: torch.Size([368])
decoder.blocks.1.conv2.1.running_var: torch.Size([368])
decoder.blocks.1.conv2.1.num_batches_tracked: torch.Size([])
decoder.blocks.1.attention2.attention.cSE.1.weight: torch.Size([23, 368, 1, 1])
decoder.blocks.1.attention2.attention.cSE.1.bias: torch.Size([23])
decoder.blocks.1.attention2.attention.cSE.3.weight: torch.Size([368, 23, 1, 1])
decoder.blocks.1.attention2.attention.cSE.3.bias: torch.Size([368])
decoder.blocks.1.attention2.attention.sSE.0.weight: torch.Size([1, 368, 1, 1])
decoder.blocks.1.attention2.attention.sSE.0.bias: torch.Size([1])
decoder.blocks.2.conv1.0.weight: torch.Size([352, 432, 3, 3])
decoder.blocks.2.conv1.1.weight: torch.Size([352])
decoder.blocks.2.conv1.1.bias: torch.Size([352])
decoder.blocks.2.conv1.1.running_mean: torch.Size([352])
decoder.blocks.2.conv1.1.running_var: torch.Size([352])
decoder.blocks.2.conv1.1.num_batches_tracked: torch.Size([])
decoder.blocks.2.attention1.attention.cSE.1.weight: torch.Size([27, 432, 1, 1])
decoder.blocks.2.attention1.attention.cSE.1.bias: torch.Size([27])
decoder.blocks.2.attention1.attention.cSE.3.weight: torch.Size([432, 27, 1, 1])
decoder.blocks.2.attention1.attention.cSE.3.bias: torch.Size([432])
decoder.blocks.2.attention1.attention.sSE.0.weight: torch.Size([1, 432, 1, 1])
decoder.blocks.2.attention1.attention.sSE.0.bias: torch.Size([1])
decoder.blocks.2.conv2.0.weight: torch.Size([352, 352, 3, 3])
decoder.blocks.2.conv2.1.weight: torch.Size([352])
decoder.blocks.2.conv2.1.bias: torch.Size([352])
decoder.blocks.2.conv2.1.running_mean: torch.Size([352])
decoder.blocks.2.conv2.1.running_var: torch.Size([352])
decoder.blocks.2.conv2.1.num_batches_tracked: torch.Size([])
decoder.blocks.2.attention2.attention.cSE.1.weight: torch.Size([22, 352, 1, 1])
decoder.blocks.2.attention2.attention.cSE.1.bias: torch.Size([22])
decoder.blocks.2.attention2.attention.cSE.3.weight: torch.Size([352, 22, 1, 1])
decoder.blocks.2.attention2.attention.cSE.3.bias: torch.Size([352])
decoder.blocks.2.attention2.attention.sSE.0.weight: torch.Size([1, 352, 1, 1])
decoder.blocks.2.attention2.attention.sSE.0.bias: torch.Size([1])
decoder.blocks.3.conv1.0.weight: torch.Size([336, 384, 3, 3])
decoder.blocks.3.conv1.1.weight: torch.Size([336])
decoder.blocks.3.conv1.1.bias: torch.Size([336])
decoder.blocks.3.conv1.1.running_mean: torch.Size([336])
decoder.blocks.3.conv1.1.running_var: torch.Size([336])
decoder.blocks.3.conv1.1.num_batches_tracked: torch.Size([])
decoder.blocks.3.attention1.attention.cSE.1.weight: torch.Size([24, 384, 1, 1])
decoder.blocks.3.attention1.attention.cSE.1.bias: torch.Size([24])
decoder.blocks.3.attention1.attention.cSE.3.weight: torch.Size([384, 24, 1, 1])
decoder.blocks.3.attention1.attention.cSE.3.bias: torch.Size([384])
decoder.blocks.3.attention1.attention.sSE.0.weight: torch.Size([1, 384, 1, 1])
decoder.blocks.3.attention1.attention.sSE.0.bias: torch.Size([1])
decoder.blocks.3.conv2.0.weight: torch.Size([336, 336, 3, 3])
decoder.blocks.3.conv2.1.weight: torch.Size([336])
decoder.blocks.3.conv2.1.bias: torch.Size([336])
decoder.blocks.3.conv2.1.running_mean: torch.Size([336])
decoder.blocks.3.conv2.1.running_var: torch.Size([336])
decoder.blocks.3.conv2.1.num_batches_tracked: torch.Size([])
decoder.blocks.3.attention2.attention.cSE.1.weight: torch.Size([21, 336, 1, 1])
decoder.blocks.3.attention2.attention.cSE.1.bias: torch.Size([21])
decoder.blocks.3.attention2.attention.cSE.3.weight: torch.Size([336, 21, 1, 1])
decoder.blocks.3.attention2.attention.cSE.3.bias: torch.Size([336])
decoder.blocks.3.attention2.attention.sSE.0.weight: torch.Size([1, 336, 1, 1])
decoder.blocks.3.attention2.attention.sSE.0.bias: torch.Size([1])
decoder.blocks.4.conv1.0.weight: torch.Size([320, 336, 3, 3])
decoder.blocks.4.conv1.1.weight: torch.Size([320])
decoder.blocks.4.conv1.1.bias: torch.Size([320])
decoder.blocks.4.conv1.1.running_mean: torch.Size([320])
decoder.blocks.4.conv1.1.running_var: torch.Size([320])
decoder.blocks.4.conv1.1.num_batches_tracked: torch.Size([])
decoder.blocks.4.attention1.attention.cSE.1.weight: torch.Size([21, 336, 1, 1])
decoder.blocks.4.attention1.attention.cSE.1.bias: torch.Size([21])
decoder.blocks.4.attention1.attention.cSE.3.weight: torch.Size([336, 21, 1, 1])
decoder.blocks.4.attention1.attention.cSE.3.bias: torch.Size([336])
decoder.blocks.4.attention1.attention.sSE.0.weight: torch.Size([1, 336, 1, 1])
decoder.blocks.4.attention1.attention.sSE.0.bias: torch.Size([1])
decoder.blocks.4.conv2.0.weight: torch.Size([320, 320, 3, 3])
decoder.blocks.4.conv2.1.weight: torch.Size([320])
decoder.blocks.4.conv2.1.bias: torch.Size([320])
decoder.blocks.4.conv2.1.running_mean: torch.Size([320])
decoder.blocks.4.conv2.1.running_var: torch.Size([320])
decoder.blocks.4.conv2.1.num_batches_tracked: torch.Size([])
decoder.blocks.4.attention2.attention.cSE.1.weight: torch.Size([20, 320, 1, 1])
decoder.blocks.4.attention2.attention.cSE.1.bias: torch.Size([20])
decoder.blocks.4.attention2.attention.cSE.3.weight: torch.Size([320, 20, 1, 1])
decoder.blocks.4.attention2.attention.cSE.3.bias: torch.Size([320])
decoder.blocks.4.attention2.attention.sSE.0.weight: torch.Size([1, 320, 1, 1])
decoder.blocks.4.attention2.attention.sSE.0.bias: torch.Size([1])
segmentation_head.weight: torch.Size([1, 320, 3, 3])
segmentation_head.bias: torch.Size([1])
